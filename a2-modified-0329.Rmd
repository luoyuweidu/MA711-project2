---
title: "MA710-A2- College Scorecard Association Rules & Cluster Analysis"
author: "Xiang Li, Xue Zhou"
date: "March 21, 2017"
output: html_document
---
# Table of Contents
* 1.[Introduction](#Introduction)
* 2.[Association Rule Analysis](#AR)
  * 2.1 [Goal](#AR_goal)
  * 2.2 [Data Preparation](#AR_dp)
  * 2.3 [Objective](#AR_objective)
  * 2.4 [Association Rule Analysis - Median earnings six years after entry](#AR_md6)
  * 2.5 [Conclusions -  Association Rules Analysis for Median earnings six years after entry](#2.5)
  * 2.6 [Association Rule Analysis - Predominant Degree](#2.6)
  * 2.7 [Conclusions -  Association Rules Analysis for Predominant Degree](#2.7)
* 3.[Cluster Analysis](#3)
  * 3.1 [Goal](#3.1)
  * 3.2 [Data Preparation](#3.2)
  * 3.3 [Objective](#3.3)
  * 3.4 [Cluster Analysis - K means](#3.4)
  * 3.5 [Cluster Analysis - PAM](#3.5)
  * 3.6 [Cluster Analysis - Hierarchical](#3.6)
  * 3.7 [Cluster Validation and Comparison](#3.7)
  * 3.8 [Interpretation for 3-cluster hierarchical solution](#3.8)
  * 3.9 [Findings and Conclusion](#3.9)
* 4 [Future Study](#4)
       
```{r setup, include=FALSE,warning=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message = FALSE)
```

# 1. Introduction<a id="Introduction"></a>

In this analysis, we used *Association Rule Analysis* and *Cluster Analysis* to study the College Scorecard Data. The analysis consists of two parts. In the first part, we used *Association Rule Analysis* to detect the co-occurring associations between different variables and identified the most important relationships. In the second part, we used *Cluster Analysis* to obtain clusters of colleges that share the common characteristics so that we could better understand the profile of different institutions.
    
# 2. Association Rule Analysis<a id="AR"></a>

## 2.1. Goal<a id="AR_goal"></a>

At the first stage(Assignment1), we've explored the relationships between pairs of variables in the College Scorecard data set. In this analysis, we mainly focused on two variables we are most interested in: the students' post-graduation earnings and the institutions' predominant degree types. We used *Association Rule Analysis* to explore the relationships between post-graduation earnings, predominant degree and the other variables. The clean data set in Assignment 1 -  ```data_clean.csv``` is the data set we used in this study. 
      
  
## 2.2 Data Preparation<a id="AR_dp"></a>

First, we loaded all the packages required for analysis as below and imported the data set.  

```{r}
  library(readr)
  library(dplyr)
  library(ggplot2)
  library(tidyr)
  library(ggvis)
  library(plyr)
  library(arules)
  library(arulesViz)
  library(cluster)
  library(clValid)

  data = read.csv("data_clean.csv",
                   header = TRUE, na.strings = 'NA')
```
  
  As seen from the code block, ```data``` is the original data set we used in this analysis
  
  ```Column X```, ```UNITID```, ```INSTNM_factor``` and ```MD_EARN_WNE_P10``` are not related to our analysis goal, so we removed these four variables. Then we renamed the column names, making them easy to understand.
  
```{r}
  data_asso = data[,-c(1,2,3,17)]
  #data_asso = na.omit(data_asso)
  data_asso %>% dplyr::rename(state = STABBR_factor,
                       pred_degree = PREDDEG_factor,
                       control = Control_factor,
                       net_cost = NPT4_COMBINE,
                       per_independent = DEP_STAT_PCT_IND,
                       per_1generation = PAR_ED_PCT_1STGEN,
                       median_family_inc =MD_FAMINC,
                       per_pell = PCTPELL,
                       per_loan = PCTFLOAN,
                       debt_grad = GRAD_DEBT_MDN,
                       debt_non_grad = WDRAW_DEBT_MDN,
                       per_app_greater2 = APPL_SCH_PCT_GE2,
                       median_earning_6years = MD_EARN_WNE_P6,
                       repayment_rate = RPY_3YR_RT_SUPP,
                       default_rate = CDR3) %>%
                       {.} -> data_asso
```

The ```data_asso``` is the resulting data set. 
  
To make the data set appropriate for *Association Rule Analysis*, we encoded all the numeric variables into categorical variables with appropriate levels. We used the following function ```make.ntiles``` to perform this conversion.

```{r}
  # create factor variables from all of the numeric variables you chose to work with using the make.ntiles function.
  make.ntiles = function (inputvar, n) {
    inputvar %>%
  quantile(.,
  (1/n) * 1:(n-1),
               na.rm=TRUE
      ) %>%
  c(-Inf, ., Inf) %>% cut(inputvar,
  breaks=.,
  paste("Q", 1:n, sep="") )
  }
```
  
  With the above function, we converted each numeric variable into a factor variable with n levels. In this case, we decide to encode all the numeric variables into 3 categories, thus n = 3. 

  Then we applied the ```make.ntiles``` function to every numeric column. After the variables were encoded, we combined them with the original three categorical variables to generate a complete data set. 
  
```{r}
  data_asso %>% 
    sapply(.,is.numeric) %>%
      data_asso[,.] %>%
        apply(.,make.ntiles, n=3, MARGIN = 2) %>%
           as.data.frame()   %>%
               {.} -> data_num
  data_clean = cbind(data_num,data_asso[,c(1,2,3)])
```
  
  The data set ```data_clean``` is the resulting data set from the above code block. In terms of the values in the encoded variables, the Q1 is the range of the low value, the Q2 is the range for the medium value and the Q3 is the range of the high value.
  
  
## 2.3 Objective<a id="AR_objective"></a> 

  Specifically, we have two two objectives in this part: 
    1.To identify the attributes which have notable relationships with the post-graduation earnings so that these variables can be used to predict the post-graduate earnings in the future. 
    2.To find the relationship between predominant degree type and other variables so that we can profile institutions of different degree types.
      
      
## 2.4 Association Rule Analysis - Median earnings six years after entry<a id="AR_md6"></a>

  Then we started generating association rules for the Median earnings six years after entry. 
  
  We used the *parameter*, *appearance* and *control* parameters of the ```apriori``` command to set the requirements for the rules.
  
  The *appearance* parameter set the right-hand side(RHS) and left-hand side(LHS) of rules. In this case, we obtained the rules with RHS limited to the values in column ```Median Earnings Six Years After Entry```.
  
  The *parameter* parameter required that all the rules must have a support value greater than 0.05 and a confidence value greater than 0.8. The *minlen* and *maxlen* parameters specified that all rules should have a length between 2 to 4. 
  
  The *control* parameter decided whether we would like to display the progress of generating rules.
  
```{r}
  #filter the rhs to the MD_EARN_WNE_P6.F column.
  apriori.appearance_e = list(rhs=c('median_earning_6years=Q1','median_earning_6years=Q2','median_earning_6years=Q3'), default='lhs')
  apriori.parameter_e = list(support=0.05,
                           confidence=0.8,minlen=2, maxlen=4)
  apriori.control_e = list(verbose=FALSE)
  rules_earning = apriori(data_clean,   
                   parameter=apriori.parameter_e,
                   appearance=apriori.appearance_e,
                   control=apriori.control_e)
  length(rules_earning)
```
  
  In total, there are 95 rules, given the parameters we have specified before. 
  
  Then we would like to remove the redundant rules. A rule is considered redundant if there exists a more general rule with the same or a higher predictive power. Since the redundant rules provide no extra information, we need to remove these rules. We used ```lift``` as a measurement of predictive power to determine the redundant rules. After removing redundant rules, we also examined the number of rules for each earnings range.
  
```{r}
  rules.sorted <- sort(rules_earning, by="lift") 
 
  ## redundant rules
  rules_redundant = rules.sorted[is.redundant(rules.sorted)]
  rules_redundant 
  
  ## non-redundant rules
  rules_pruned = rules.sorted[!is.redundant(rules.sorted)] 
  rules_pruned 
  
  ##The number of rules associated with rhs of median_earning_6years=Q1
  rules_Q1 = subset(rules_pruned, subset = items %in% "median_earning_6years=Q1")
  length(rules_Q1)
  
  ##The number of rules associated with rhs of median_earning_6years=Q2
  rules_Q2 = subset(rules_pruned, subset = items %in% "median_earning_6years=Q2")
  length(rules_Q2)
  
  ##The number of rules associated with rhs of median_earning_6years=Q3
  rules_Q3 = subset(rules_pruned, subset = items  %in% "median_earning_6years=Q3")
  length(rules_Q3)
```
   
   From the output, there are 8 redundant rules, so we only focused on the remaining 87 rules. Among the 87 valid rules, 84 of them have a RHS of ```median_earning_6years=Q3```, 3 of them have the RHS of ```median_earning_6years=Q2```, while no rules contain the RHS of  ```median_earning_6years=Q1```. 
  
  In the following part, we sorted the rules by support, confidence and lift respectively and looked at them more closely.
  
 With the following code, we sorted the resulting rules by `support`. We then inspected the top 5 rules. A balloon graph visualizing the top 5 rules was also displayed. 
 
```{r}
  #sort by support 
  rules_support = sort(rules_pruned,by="support", decreasing = T)
  inspect(rules_support[1:5])
  
  plot(head(sort(rules_pruned, by="support"), 5),
    method="grouped")
  
```

Based on the output, the RHS of the rules with highest support value are all ```median_earning_6years=Q3```. Let's take the first rule as an example. The support value of 0.1256 is the frequency of the this rule, which means that in the data set, 12.56% of the universities have high median family incomes, high debt amount before graduation, high repayment rate and high median earnings 6 years after they were enrolled. 

The confidence value is 0.8091, indicating that 80.91% of the universities whose students have high median family income, high debt amount before graduation and high repayment rate have students with high median earning 6 years after the enrollment. 

The lift value as high as 3.23 means that the high median family income, high debt, high post-graduation earnings and high repayment rate are 2 times more likely to occur together than the probability we would expect if the LHS and the RHS are independent. 

The high  support value of these rules shows that the rules are more likely to generalize well in other institutions.In summary, institutions with less students receiving pell grant, higher repayment rate and less percentage of first generation students tend to have students making more money after graduation. It's surprising that family incomes don't exist in the top rules.

Then we ordered the 87 rules by confidence and pulled out the top 5 rules. The code below generates a balloon graph visualizing these 5 rules.  

```{r}
  rules_confidence = sort(rules_pruned,by="confidence", decreasing = T)
  inspect(rules_confidence[1:5])
  
  plot(head(sort(rules_pruned, by="confidence"), 5),
    method="grouped")
```

Generally speaking, a high confidence value indicates that the frequency of the right-hand side occurring given the left-hand side occurs is high. So rules here would be useful to predict right-hand side if we already know the left-hand side.

Since most rules have RHS ```median_earning_6years=Q3```, all these five rules indicates the relationship between post-graduation earnings and the other variables. Tops Rules here are slightly different from the rules sorted by support, but still center on the factors including the pell grant, the debt amount and the percentage of the first generation student.
  
Lastly, we sort the rules by lift value and further inspect the top 5 rules. 

```{r}
  rules_lift = sort(rules_earning,by="lift", decreasing = T)
  inspect(rules_lift[1:5])
```

From the output, the rule with the highest lift value is identical with the rule with the highest confidence value (as we can tell from the previous session), which means that there exists a notable relationship between LHS (low Pell percentage value, high debt before graduation and low default rate value and RHS (high median post-graduation earnings after enrollment). Likewise, other top rules sorted by lift are also centered on factors including ```pell grant```, ```debt amount```, ```if first generation student``` and ```repayment condition```.
  
  
## 2.5 Conclusions -  Association Rules Analysis for Median earning six years after entry<a id="2.5"></a> 

Based on the above analysis, we can conclude that if a college has a low percentage of Pell Grant students, a low default rate, a high repayment rate and students from a well-educated family background, its students are more likely to have high earnings after graduation. Thus we identified some potential variables predicting students' post-graduation earnings in a college: ```percentage of pell grant students```, ```default rate```, ```repayment rate```, ```family income``` and ```the percentage of the first-generation students```. 

  
## 2.6 Association Rule Analysis - Predominant Degree<a id="2.6"></a>

  The second part of association rule analysis was focused on the relationship between the predominant degree type and the other variables. As the code below shows, we limited the RHS to the three levels of the predominant degree type. Additionally, we set the minimum support to 0.1, which means that the algorithm would only keep the rules with a frequency greater than 10%. Likewise, the minimum confidence was set to 0.8, which means that we would only keep the rules with the conditional probability greater than 0.8. 
  
```{r}
  apriori.appearance = list(rhs = c("pred_degree=Bachelor's-degree","pred_degree=Certificate-degree",
                                    "pred_degree=Associate's-degree","pred_degree=NotClassified",
                                    "pred_degree=Graduate-degree"),
                            default = 'lhs')

  apriori.parameter = list(support = 0.10,
                            confidence =0.8)
  apriori.control = list(verbose = FALSE)
  rules_degree = apriori(data_clean,
                  parameter = apriori.parameter,
                  appearance = apriori.appearance,
                  control = apriori.control)
  length(rules_degree)  
```

In total, we got 225 association rules. 

Like what we did in the 2.4 session, we used *lift* to determine and remove the redundant rules. 

```{r}
  
  rules_degree_sorted <- sort(rules_degree, by="lift") 
  length(rules_degree_sorted)
  
  ## redundant rules
  rules_degree_redundant = rules_degree_sorted[is.redundant(rules_degree_sorted)]
  
  ## non-redundant rules
  rules_degree_pruned = rules_degree_sorted[!is.redundant(rules_degree_sorted)] 
  
  rules_degree_redundant 
  rules_degree_pruned
```

After removing the redundant rules, we got the 195 rules. We further analyzed these rules in the following sections.

With the following code block, we first sorted the rules by their support, we then examined the top 5 rules. 

```{r}
  inspect(sort(rules_degree_pruned,by='support')[1:5])
```

Among top five rules sorted by support, four rules are regarding Bachelor degree while the very top rule is for certificate degree. For the first rule, a support value as high as 19% means that around 19% of institutions in the data set satisfy the following characteristics: private for-profit, certificate-predominant and with students having low debts. As we discussed before, a high support value shows that the rule is more likely to generalize well in other institutions.

The other four rules regarding Bachelor degree indicate that if most students in a school are from a wealthy and educated family and willing to apply for more debts, the school is more likely to belong to Bachelor degree predominant type.

Then we looked at the rules sorted by confidence. Likewise, we would only focus on the top 5 rules. 

```{r}
  inspect(sort(rules_degree_pruned,by='confidence')[1:5])
```

Top 5 rules are all about Bachelor degree. A high confidence indicates a strong sequential relationship between LHS and RHS, so we can say that if we observe the existence of LHS, the RHS is very likely to happen. For example, for the first rule, if a institution has low percentage of first-generation students, low percentage of students receiving the pell grant, high median family income, good repayment condition and more students applying to more than 2 school, this institution is 97% likely to provide Bachelor degree as predominant degree.

We then sorted all 195 rules by the lift value. If the lift value equals to 1, it would imply that the probability of the occurrence of LHS and the probability of occurrence of the RHS are independent. With that said, the higher the lift value is, the more likely the LHS and RHS are associated.

```{r}
  inspect(sort(rules_degree_pruned,by='lift')[1:5])
```

Let's take a look at top five rules sorted by lift. These rules all have a lift greater than 3.5, which means that these rules are about 2.5 times more likely to occur than we would expect if LHS and RHS are independent.

Based on the output, some rules with the high confidence value also have very high lift values(as we can tell from the previous discussions). A rule with a high confidence and lift value indicates a strong relationship between LHS and RHS.
  
In the following session, we used some visualizations to better understand the total 195 rules. We drew the scatter plot to visualize the relationships between support, confidence and lift.

```{r}
  plot(rules_degree_pruned, method = NULL, measure = "support", shading = "lift",
        interactive = FALSE, data = NULL, control = NULL)
```

  The plot shows a roughly negative relationship between confidence and support. However, the relationship between lift and the other two is not obvious.
  

## 2.7 Conclusions - Association Rules Analysis for Predominant Degree<a id="2.7"></a>

  As a conclusion, all the rules we found in this session describes the institutions whose predominant degree are either bachelor or certificate. No explicit association was detected between other degree types and the variables. If most students in a school are from a wealthy and  well-educated family, taking more debt for their education, and less likely to apply for Pell grant, their institutions are likely to be bachelor-predominant.

   Moreover, if an institution is a private for-profit school whose students usually have lower debt amount before graduation and lower earnings six years after graduation, the school is more likely to be certificate-predominant.
  
  
# 3. Cluster Analysis<a id="3"></a>
## 3.1 Goal<a id="3.1"></a>

  By performing Cluster Analysis on College Scoreboard data, we would like to find the groups of similar institutions and to characterize the institutions in these groups. The clustering result can help us better understand the profiles of institutions and how they are different from each other.    
     
## 3.2 Data Preparation<a id="3.2"></a>

  In the following analysis, we used the ```data_clean``` data set in assignment 1 but we did some transformation on this data set to make it appropriate for Clustering Analysis. 
  
  ```University name```, ```ID```, and ```state columns``` are attributes not useful in Clustering Analysis since they are either identifiers or variables with too many levels, thus we removed these three columns along with the meaningless index column generated by R when importing data.
  
```{R}
  data_c = read.csv("data_clean.csv",
                   header = TRUE, na.strings = 'NA')
  
  #get rid of ID, university name and state columns, rename the rownames as the university ID. 
  data.with.rownames <- data.frame(data_c[,-c(1:4)], row.names=data_c[,2])
  glimpse(data.with.rownames)
```
  
  The resulting ```data_c``` data set contains 15 variables with 7793 observations. 
  Since most distance measurements, like Euclidean Distance and Cosine Distance, are only valid for numeric variables, we converted the factor variables into binary dummy variables. 
  
```{R}
  # Create the dummy boolean variables using the model.matrix() function.
  dummy_preddeg = model.matrix(~PREDDEG_factor-1, data.with.rownames)
  dummy_control = model.matrix(~Control_factor-1, data.with.rownames)
```
   
   The ```dummy_preddeg``` and ```dummy_control``` are dummy variables for ```PREDDEG_factor``` and ```Control_factor```, with number of columns equal to number of levels of each original variable.
  
  
   In order to make the column name of 8 dummy variables easy to understand, we renamed these variables with the appropriate names and then combined them with the original 13 continuous variables. We also noticed some variables had too many missing values, so we removed the records with any missing value. 
   
```{r}
  #rename the coloumn names for dummay variables to make them more readable.
  colnames(dummy_preddeg) <- gsub("PREDDEG_factor","",colnames(dummy_preddeg))
  colnames(dummy_control) <- gsub("Control_factor","",colnames(dummy_control))
  
  #Combine the matrix back with the original dataframe.
  data_combine_c= cbind(data.with.rownames, dummy_preddeg,dummy_control) 
  
  #git rid of the factor coloumns which have been converted to the dummy variable.
  data_ready = data_combine_c[,-c(1:2)]
  
  #remove the missing values
  data_ready %>%
    na.omit(data_ready) %>%
    {.} -> data_clean_c
```
  
  In the data set ```data_clean_c```, all variables are now numeric and appropriate for Clustering Analysis. 
  
   To ensure that every variable receives the same weight, we then standardized the numeric variables in the data set. We also noticed the ```Graduate-degree``` only contained 0 values, which means that it couldn't provide any additional information. So, we removed this column.
   
```{R}
  #delete the 17 col since its all 0 
  data_final= data.frame(scale(data_clean_c[,-17]))
```
   
   The resulting ```data_final``` is the clean data set for analysis, with 4528 observations and 20 columns. 
  
  
## 3.3 Objective<a id="3.3"></a>

   As we mentioned before, the goal of this Clustering Analysis is to understand the profile of institutions and to see how they are different from each other. More specifically, we have following three objectives:
   
   1. For each cluster, to create a concrete profile for institutions in this clusters.
   
   2. To identify variables that differentiate one cluster from another.
   
   3. To see if some characteristics tend to occur together (like high repayment usually comes with low default rate).
   
   In the following analysis, we are going to use three clustering methods: K-means, PAM and Hierarchical. For each method, we identified the best number of clusters. And then we will compared the different methods using ```clValid``` function to select the best model.
  
## 3.4 Cluster Analysis process - K-means<a id="3.4"></a>

  We set the seed value to 100 so that we could reproduce the analysis.
  
```{r}
  set.seed(100)
```
    
  To determine the optimum k value for clustering the observations, we used scree plots to display the ratio of WSS and TSS for 1 cluster to 7 clusters. 
  
```{R}
  # Initialise ratio_ss
  ratio_ss <- rep(0, 7)
  
  # Finish the for-loop
  for (k in 1:7) {
    # Apply k-means to data_final: data_km
     data_km <- kmeans(data_final, k, nstart = 20)
    # Save the ratio between of WSS to TSS in kth element of ratio_ss
    ratio_ss[k] <- data_km$tot.withinss / data_km$totss
  }
  # Make a scree plot with type "b" and xlab "k"
  plot(ratio_ss, type = "b", xlab = "k")
  
```
    
    
    From the scree plot, we can tell that the 2-cluster or 3-cluster solution is worth trying. We chose k=3 for the  detailed analysis here but we would consider the 2-cluster solution in the evaluation stage using `clValid`. 
  
  
  Then we used k means function with the cluster number of 3, ```nstart =20``` means that the algorithm attempts 20 initial trials and returns the best clustering result. 
  
```{R}
  km_result <- kmeans(data_final, 3, nstart = 20)
  
  data_final%>%
    mutate(cluster = factor(km_result$cluster)) %>%
     {.} -> data_km
```
  
  The corresponding cluster solution is saved in a new column named ```cluster```. The new data set is named as ```data_km```.
  
  To better understand the size of each cluster, we used the ```dplyr``` function to produce some summary statistics.
  
```{R}
    data_km %>%
    group_by(cluster) %>%
       dplyr::summarise(COUNT = n()) 
```
  
  From the output, we see that cluster 1, 2, 3 contain  946, 1,440 and 2,142 institutions respectively. 
  
## 3.5 Cluster Analysis process - Partitioning around mediods (PAM)<a id="3.5"></a>
  Then we used the PAM method.
  
  To determine an optimal number of clusters for Partitioning around mediods, we calculated the *silhouette* width for cluster numbers from 2 to 10 and plotted a scree plot. 
  
```{r}
  # Calculate silhouette width for many k using PAM
  sil_width <- c(NA)
  
  for(i in 2:10){
    pam_fit <- pam(data_final,diss=FALSE,
                   k = i)
    sil_width[i] <- pam_fit$silinfo$avg.width
  }
  
  plot(1:10, sil_width,
       xlab = "Number of clusters",
       ylab = "Silhouette Width")
  lines(1:10, sil_width)
```
  
  From the scree plot, we can see the 2-cluster solution has the greatest Silhouette Width values 0.32. Still, the Silhouette Width is very low, which suggests the observations were not effectively clustered into different groups.
  
  
## 3.6 Cluster Analysis process - Hierarchical<a id="3.6"></a> 
  The last method we used was Hierarchical Clustering Analysis.
  
  We used the ```dist``` function to calculate the Euclidean distance for our observations and then used ```hclust``` function to cluster the observations. 
  
```{r}
  hclust_result = hclust(dist(data_final))
  plot(hclust_result, label=data_final$NPT4_Pulic)
```

A dengogram was also generated to help select the number of clusters. Due to the large size of our data set, there is not much useful information we could extract from the bottom part of the dengogram. However, there are clearly two very distinct groups at the top of the dengogram. It looks like either two or three groups might be an good place to start investigating. 

  
  We tried both 2-cluster and 3-cluster solutions as follows: 
  
```{r}
  data_final %>%
    mutate(hc_cluster2 = factor(cutree(hclust_result,k=2)),hc_cluster3 =factor(cutree(hclust_result,k=3)) ) %>%
     {.} -> data_hc
```
  
  The clustering result for each observation was stored in two new columns named ```hc_cluster2``` and ```hc_cluster3```, the updated data set was named with ```data_hc```. Before we interpret the cluster solutions, we used ```clValid``` function in the following section to validate and finalize the optimum cluster solution.

  
## 3.7 Cluster Validations and Comparisons<a id="3.7"></a> 
  
To compare the different clustering methods so as to choose the optimum number of clusters, we ran a cluster validation using ```clValid``` function. This function allows us to compare the multiple clustering evaluation metrics under different clustering methods as well as different cluster numbers.

```{r}
  #rownames(data_final) = 1:4528
  methods.vec = c("hierarchical","kmeans","pam")
  clValid.result = clValid(data_final,
                           2:5,
                           clMethods=methods.vec,
                           validation="internal",
                           maxitems = 2000000)
  summary(clValid.result)
```

The result shows that the optimum number of cluster is 2, and the best method is hierarchical. 

  
In the following session, we first analyzed the 2-cluster solution in details. 

```{r}
  #two cluster solutions 
  data_hc %>%
    group_by(hc_cluster2) %>%
       dplyr::summarise(COUNT = n()) 
```

From the output for the two cluster solutions, the first cluster has 4524 observations while the second cluster has only 4 observations. The unbalanced distribution makes the clustering result less meaningful because the profile of second cluster is not general enough. Therefore, after examining the table produced by ```clValid``` above, we decided to select the hierarchical method with 3-cluster solution as our final cluster solution. 
  
  
## 3.8 The Interpretation for 3 cluster hierachical solutions<a id="3.8"></a> 

We used the following code to generate the histogram for hierarchical 3-Cluster Solution.

```{r}
#three cluster solutions 
data_hc %>%
    group_by(hc_cluster3) %>%
       dplyr::summarise(COUNT = n())

data_hc %>%
  group_by(hc_cluster3) %>%
     dplyr::summarise(COUNT = n()) %>%
       ggplot(aes(x = hc_cluster3, y = COUNT)) + geom_bar(stat = "identity") + ggtitle('Histogram for Hierachical 3 Cluster Solutions') + theme(plot.title = element_text(family = "Trebuchet MS", color="#666666", face="bold", size=12, hjust=0.5)) 
```

Based on the output, the first cluster has 2,992 observations, the second cluster has 1,532 observations while the third cluster has 4 observations. 

We first looked at the third cluster to see what made these observations different from others.

```{r}
#select the cluster 3 membership from the original dataset

cluster3 = subset(data_c, UNITID %in% rownames(subset(data_hc,hc_cluster3==3)))[,-1]
cluster3[,1:5]
```
 
 Based on the output, we can tell that all of these four universities are all private for-profit with the predominant degree as *Not-Classified*. Institutions in this cluster have a higher percent of pell grant, higher loan amount and a higher percent of first-generation students compared to the other two clusters. 
 
Then We calculated the median value of each column grouped by cluster.

```{r}
#three cluster solution summary statistics 
data_hc[,-21]%>%
  group_by(hc_cluster3) %>%
     dplyr::summarise_each(funs(median)) 
```

We can tell that the schools in cluster 1 are the TITLE IV institutions with low average net price, high percentage of first-generation students, low median family income, high percentage of Pell Grant receiving students, high federal loan, low median debt, low median income six years after entry, low median income ten years after entry and low repayment rate 3 years after entering repayment. **To summarize, students in these institution tend to from a less wealthy background, who need more financial aid and make less money after graduation**

Institutions in cluster 2 are opposite to those in cluster 1. The students from these institutions are from a more well-educated and wealthy family and are more willing to invest money in education through student loan or family support. These students also make more money after graduation.

Then we created some visualizations to help us explore the relationships between variables in each cluster. First we plotted ```earnings``` against ```percentage of Pell Grant```.

```{R}
#ggplot
data_hc %>% 
  ggplot(aes(x=MD_EARN_WNE_P6, y=PCTPELL)) + 
  geom_point(aes(color=hc_cluster3)) + 
  guides(size=FALSE) + ggtitle('Scatter Plot for PCTPELL against MD_EARN_WNE_P under Hierachical 3 cluster solutions ') + theme(plot.title = element_text(family = "Trebuchet MS", color="#666666", face="bold", size=8, hjust=0.5)) +
    xlab("Earnings six years after entry")+
    ylab("Percentage of Pell Grant")
```
  
   From the scatter plot, it is easy to tell that the institutions in the cluster 1 have more students who receive the Pell Grant meanwhile the students' median earnings 6 years after the enrollment are relative low compared to the institutions from the cluster 2. 


Likewise, we explored the distributions about earnings for each cluster as below: 

```{r}
data_hc %>%
  ggplot(aes(x = hc_cluster3,
             y = MD_EARN_WNE_P6)) +
  geom_boxplot(aes(fill = hc_cluster3)) +
  xlab("Cluster") +
  ylab("Median Earning six years after entry") +
  scale_fill_discrete(name = 'Cluster') 
```

   It's evident that the institutions in cluster 2 have higher earnings than those in cluster 1 and 3. Moreover, about 70% of values in cluster 1 are above the average earning for all the observations. The earning values for cluster 1 and 3 roughly fall in a similar range while the observations in cluster 1 spread more widely. In addition, most of the observations in cluster 1 and 3 have an earning value below the average.

We also plotted median family income against earnings after six years of entry for each cluster.

```{r}
qplot(data = data_hc,
      x = MD_FAMINC,
      y = MD_EARN_WNE_P6,
      color = hc_cluster3,
      xlab = 'Median Family Income',
      ylab = "Median Earning six years after entry",
      main = "Scatter plot between Median Earning six years after entry and Median Family Income and")
```
  
    
    We see that the most observations in cluster 2 have a higher value for both `Median Family Income` and `Median Earning six years after entry`.


## 3.9 Findings and Conclusions<a id="3.9"></a> 

To find the intrinsic groups in the College Scorecard data set, we have used K-means, PAM and Hierarchical methods to perform clustering analysis. The cluster solutions for each method were different so we used metrics such as *Connectivity*, *Dunn Index* and *Silhouette width* to evaluate the clustering performance. 

The optimal clustering solution we found was **Hierarchical method with 3 clusters**. When selecting the optimal clustering solution, not only did we take into account the clusters performance indicated by evaluation metrics but  also we used human judgement on the meaningfulness of the clustering result. As a result, we dropped the best result generated by computer - Hierarchical with 2 clusters, and go with the Hierarchical with 3 clusters.

The final three clusters contain 2,992 observations, 1,532 observations and 4 observations respectively. It's unusual that one cluster only contains 4 observation, which may not be able to provide us with meaningful insights. In the future, an alternative study we would consider is to treat these 4 observations as outliers and do another cluster analysis without these 4 observations, and then compare if the two results are different.

The result shows that the students from cluster 1 institutions are more likely to from less wealthier family and more likely to apply for pell grant. These students also tend to make less money after graduation. In contrast, the students from cluster 2 institutions are more likely to from well-educated and wealthy families, and make more money after graduation. 


# 4.Further Studies<a id="4"></a>

The association rules we have generated from this study can be used to determine the candidate predictors for predicting colleges' post-graduation earnings in the future stages. Likewise, clustering result also can be used as a new input in the predictive model.

In terms of the solution we have selected for the cluster analysis, the results could be improved by investigating on the 4 observations(the observations in the cluster 3 generated by the computer). The analysis process covered in this paper could be revised if the 4 observations in the cluster 3 are outliers. Moreover, a density-based cluster method like DBSCAN can be included in the analysis process as well.




