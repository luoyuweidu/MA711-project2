---
title: "MA710-A2- College Scorecard Association Rules & Cluster Analysis"
author: "Xiang Li, Xue Zhou"
date: "March 21, 2017"
output: html_document
---
# Table of Contents
* 1.[Introduction](#Introduction)
* 2.[Association Rule Analysis](#AR)
  * 2.1 [Goal](#AR_goal)
  * 2.2 [Data Preparation](#AR_dp)
  * 2.3 [Objective](#AR_objective)
  * 2.4 [Association Rule Analysis - Median earnings six years after entry](#AR_md6)
  * 2.5 [Conclusions -  Association Rules Analysis for Median earnings six years after entry](#2.5)
  * 2.6 [Association Rule Analysis - Predominant Degree](#2.6)
  * 2.7 [Conclusions -  Association Rules Analysis for Predominant Degree](#2.7)
* 3.[Cluster Analysis](#3)
  * 3.1 [Goal](#3.1)
  * 3.2 [Data Preparation](#3.2)
  * 3.3 [Objective](#3.3)
  * 3.4 [Cluster Analysis - K means](#3.4)
  * 3.5 [Cluster Analysis - PAM](#3.5)
  * 3.6 [Cluster Analysis - Hierarchical](#3.6)
  * 3.7 [Cluster Validation and Comparison](#3.7)
  * 3.8 [Interpretation for 3-cluster hierarchical solution](#3.8)
  * 3.9 [Findings and Conclusion](#3.9)
* 4 [Future Study](#4)
       
```{r setup, include=FALSE,warning=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message = FALSE)
```

# 1. Introduction<a id="Introduction"></a>

In this analysis, we use *Association Rule Analysis* and *Cluster Analysis* to study the College Scorecard Data. The analysis consists of two parts. In the first part, we use *Association Rule Analysis* to detect the co-occurring associations between different variables and identify the most important relationships. In the second part, we use *Cluster Analysis* to obtain clusters of colleges that share the common characteristics so that we are able to better understand the profile of different institutions.
    
# 2. Association Rule Analysis<a id="AR"></a>

## 2.1. Goal<a id="AR_goal"></a>

At the first stage(Assignment1), we've explored the relationship between pairs of variables in the College Scorecard data set. In this analysis, we mainly focus on two variables we are most interested in: the students' post-graduation earnings and the institutions' predominant degree types. We use *Association Rule Analysis* to explore relationship between post-graduation earnings, predominant degree and the other variables. The clean data set in Assignment 1 -  ```data_clean.csv``` is the data set we use in this study. 
      
  
## 2.2 Data Preparation<a id="AR_dp"></a>

First, we load all the packages required for analysis as below and import the data set.  

```{r}
  library(readr)
  library(dplyr)
  library(ggplot2)
  library(tidyr)
  library(ggvis)
  library(plyr)
  library(arules)
  library(arulesViz)
  library(cluster)
  library(clValid)

  data = read.csv("data_clean.csv",
                   header = TRUE, na.strings = 'NA')
```
  
  As seen from the code block, ```data``` is the original data set we use in this analysis
  
  Column X, UNITID, INSTNM_factor and MD_EARN_WNE_P10 are not related to our analysis goal, we remove these four variables. Then we rename the column names, making them easy to understand.
  
```{r}
  data_asso = data[,-c(1,2,3,17)]
  #data_asso = na.omit(data_asso)
  data_asso %>% dplyr::rename(state = STABBR_factor,
                       pred_degree = PREDDEG_factor,
                       control = Control_factor,
                       net_cost = NPT4_COMBINE,
                       per_independent = DEP_STAT_PCT_IND,
                       per_1generation = PAR_ED_PCT_1STGEN,
                       median_family_inc =MD_FAMINC,
                       per_pell = PCTPELL,
                       per_loan = PCTFLOAN,
                       debt_grad = GRAD_DEBT_MDN,
                       debt_non_grad = WDRAW_DEBT_MDN,
                       per_app_greater2 = APPL_SCH_PCT_GE2,
                       median_earning_6years = MD_EARN_WNE_P6,
                       repayment_rate = RPY_3YR_RT_SUPP,
                       default_rate = CDR3) %>%
                       {.} -> data_asso
```

The ```data_asso``` is the resulting data set. 
  
To make the data set appropriate for *Association Rule Analysis*, we encode all the numeric variables into categorical variables with appropriate levels. We use the following function *make.ntiles* to perform this conversion.

```{r}
  # create factor variables from all of the numeric variables you chose to work with using the make.ntiles function.
  make.ntiles = function (inputvar, n) {
    inputvar %>%
  quantile(.,
  (1/n) * 1:(n-1),
               na.rm=TRUE
      ) %>%
  c(-Inf, ., Inf) %>% cut(inputvar,
  breaks=.,
  paste("Q", 1:n, sep="") )
  }
```
  
  With the above function, we are able to convert each numeric variable into a factor variable with n levels. In this case, we decide to encode all the numeric variables into 3 categories, thus n = 3. 

  Then we apply the ```make.ntiles``` function to every numeric column. After the variables are encoded, we combine the them with the original three categorical variables to generate a complete data set. 
  
```{r}
  data_asso %>% 
    sapply(.,is.numeric) %>%
      data_asso[,.] %>%
        apply(.,make.ntiles, n=3, MARGIN = 2) %>%
           as.data.frame()   %>%
               {.} -> data_num
  data_clean = cbind(data_num,data_asso[,c(1,2,3)])
```
  
  The data set ```data_clean``` is the resulting data set from the above code block. In terms of the values in the encoded variables, we assume that the Q1 is the range of the low value, the Q2 is the range for the medium value and the Q3 is the range of the high value.
  
  
## 2.3 Objective<a id="AR_objective"></a> 

  Specifically, we had two two objectives in this part: 
        1. Identify the attributes which have notable relationships with the post-graduation earnings so that these variables can be used to predict the post-graduate earnings in the future. 
        2. Find the relationship between predominant degree type and other variables so that we can profile insitutions of different degree types.
      
      
## 2.4 Association Rule Analysis - Median earnings six years after entry<a id="AR_md6"></a>

  Then we start generating association rules for the Median earnings six years after entry. 
  
  We use the parameter, appearance and control parameters of the ```apriori``` command to set the requirements for the rules.
  
  The *appearance* parameter set the right-hand side(rhs) and left-hand side(lhs) of rules. In this case, we only need rules with a rhs relevant to Median earnings six years after entry.
  
  The *parameter* parameter requires that all the rules must have a support value greater than 0.05 and a confidence value greater than 0.8. The *minlen* and *maxlen* parameters specify that all rules should have a length between 2 to 4. 
  
  The *control* parameter decide if we want to show the progress of generating rules.
  
```{r}
  #filter the rhs to the MD_EARN_WNE_P6.F column.
  apriori.appearance_e = list(rhs=c('median_earning_6years=Q1','median_earning_6years=Q2','median_earning_6years=Q3'), default='lhs')
  apriori.parameter_e = list(support=0.05,
                           confidence=0.8,minlen=2, maxlen=4)
  apriori.control_e = list(verbose=FALSE)
  rules_earning = apriori(data_clean,   
                   parameter=apriori.parameter_e,
                   appearance=apriori.appearance_e,
                   control=apriori.control_e)
  length(rules_earning)
```
  
  In total, there are 95 rules, given the parameters we specify before. 
  
  Then we would like to remove redundant rules. A rule is considered redundant if there exists a more general rule with the same a higher predictive power. Since the redundant rules provide no extra information, we want to remove these rules. We use ```lift``` as a measurement of predictive power to determine the redundant rules. After removing redundant rules, we also calculate the number of rules for each earnings range.
  
```{r}
  rules.sorted <- sort(rules_earning, by="lift") 
 
  ## redundant rules
  rules_redundant = rules.sorted[is.redundant(rules.sorted)]
  rules_redundant 
  
  ## non-redundant rules
  rules_pruned = rules.sorted[!is.redundant(rules.sorted)] 
  rules_pruned 
  
  ##The number of rules associated with rhs of median_earning_6years=Q1
  rules_Q1 = subset(rules_pruned, subset = items %in% "median_earning_6years=Q1")
  length(rules_Q1)
  
  ##The number of rules associated with rhs of median_earning_6years=Q2
  rules_Q2 = subset(rules_pruned, subset = items %in% "median_earning_6years=Q2")
  length(rules_Q2)
  
  ##The number of rules associated with rhs of median_earning_6years=Q3
  rules_Q3 = subset(rules_pruned, subset = items  %in% "median_earning_6years=Q3")
  length(rules_Q3)
```
   
   From the output, there are 8 redundant rules, so we will only focus on the remaining 87 rules. Among the 87 valid rules, 84 of them have the lhs of ```median_earning_6years=Q3```, 3 of them have the `RHS` of ```median_earning_6years=Q2```, while no rules contain the `RHS` of  ```median_earning_6years=Q1```. 
  
  In the following part, we are going to sort the rules by support, confidence and lift repectively and look at them more closely.
  
 With the following code, we sort the resulting rules by `support`. We then inspect the top 5 rules. A balloon graph visualizaing the top 5 rules is also displayed. 
 
```{r}
  #sort by support 
  rules_support = sort(rules_pruned,by="support", decreasing = T)
  inspect(rules_support[1:5])
  
  plot(head(sort(rules_pruned, by="support"), 5),
    method="grouped")
  
```

Based on the output, the rules with the highest support value all have a ```median_earning_6years=Q3``` as ```rhs```. Let's take the first rule as an example. The support value of 0.1256 is the frequency of the this rule, which means that in the data set, 12.56% of the universities have high median family income, high debt amount before graduation, high repayment rate and high median earnings 6 years after they were enrolled. 

The confidence value is 0.8091, indicating that 80.91% of the universities whose students have high median family income, high debt amount before graduation and high repayment rate have students with high median earning 6 years after the enrollment. 

The lift value as high as 3.23 means that high median family income, high debt, high post-graduation earnings and high repayment rate are 2 times more likely to occur together than we would expect if the lhs and the rhs are independent. 

The high value of support of these rules shows that they are more likely to generalize well on other institutions.In summary, institutions with less students applying for pell grant, higher repayment rate and less percentage of first generation students tend to have student making more money after graduation. It's surprising that family incomes doesn't exist in the top rules.

Then we order the 87 rules by confidence and pull out the top 5 rules. The code below generates a balloon graph visualizing these 5 rules.  

```{r}
  rules_confidence = sort(rules_pruned,by="confidence", decreasing = T)
  inspect(rules_confidence[1:5])
  
  plot(head(sort(rules_pruned, by="confidence"), 5),
    method="grouped")
```

Generally speaking, a high confidence value indicates that the frequency of the right-hand side occuring given the left-hand side occurs is high. So rules here would be useful to predict right-hand side if we already know the left-hand side.

Since most rules with have right-hand side ```median_earning_6years=Q3```, all these five rules are all releted to high post-graduation earnings. Tops Rules here are slightly different from the rules sorted by support, but still center on the factors incluing pell grant, debt amount and if first generation student.
  
Lastly, we sort the rules by lift value and further inspect the top 5 rules. 

```{r}
  rules_lift = sort(rules_earning,by="lift", decreasing = T)
  inspect(rules_lift[1:5])
```

From the output,the rule with the highest lift value is identical with the rule with the highest confidence value(as we can tell from the previous session), which means that there exists a notable relationship between lhs(low Pell percentage value, high debt before graduation and low default rate value and rhs(high median post-graduation earnings after enrollment). Likewise, other top rules sorted by lift are also centered on incluing ```pell grant```, ```debt amount``` and ```if first generation student```, repayment condition.
  
  
## 2.5 Conclusions -  Association Rules Analysis for Median earning six years after entry<a id="2.5"></a> 

Based on the above analysis, we can conclude that if a college has a low percentage of Pell Grant students, a low default rate, a high repayment rate and students from a well-educated family background, its students are more likely to have a high earning after graduation. Thus we are able to identify some potential variables predicting students' post-graduation earnings in a college: **percentage of pell grant students**, **default rate**, **repayment rate**, **family income** and **the percentage of the first-generation students**. 

  
## 2.6 Association Rule Analysis - Predominant Degree<a id="2.6"></a>

  The second part of association rule analysis is focused on the relationship between predominant degree type and the other variables. As the codes below shows, we limit the ```rhs``` to the three levels of the predominant degree type. Additionally, we set the minimum support to 0.1, which means that the algorithm will only keep the rules with a frequency greater than 10%. Likewise, the minimum confidence is set to 0.8, which means that we will only keep the rules with the conditional probability greater than 0.8. 
  
```{r}
  apriori.appearance = list(rhs = c("pred_degree=Bachelor's-degree","pred_degree=Certificate-degree",
                                    "pred_degree=Associate's-degree","pred_degree=NotClassified",
                                    "pred_degree=Graduate-degree"),
                            default = 'lhs')

  apriori.parameter = list(support = 0.10,
                            confidence =0.8)
  apriori.control = list(verbose = FALSE)
  rules_degree = apriori(data_clean,
                  parameter = apriori.parameter,
                  appearance = apriori.appearance,
                  control = apriori.control)
  length(rules_degree)  
```

In total, we get 225 association rules. 

Like what we do in the 2.4 session, we use ```lift``` measure to determine and remove the redundant rules. 

```{r}
  
  rules_degree_sorted <- sort(rules_degree, by="lift") 
  length(rules_degree_sorted)
  
  ## redundant rules
  rules_degree_redundant = rules_degree_sorted[is.redundant(rules_degree_sorted)]
  
  ## non-redundant rules
  rules_degree_pruned = rules_degree_sorted[!is.redundant(rules_degree_sorted)] 
  
  rules_degree_redundant 
  rules_degree_pruned
```

After removing the redundant rules, we get the 195 rules. We will further analyze these rules in the following sections.

We first sort the rules by their support with the following code block, we then look at top 5 rules. 

```{r}
  inspect(sort(rules_degree_pruned,by='support')[1:5])
```

Among top five rules sorted by support, four rules are related to Bachelor degree while the very top rule is for certificate degree. For the first rule, a support value as high as 19% means that around 19% of institutions in the data set are private for-profit institutions awarding certificate degree predomintly and with attending students having low debts. As we discuss before, a high support value shows that the rule is more likely to generalize well on other institutions.

The other four rules related to Bachelor degree indicate that if students in a school are from a wealthier and educated family, willing to apply for more debts, the school is more likely to belong to Bachelor degree predominant type.

Then we look at the rules sorted by confidence. Likewise, we would only focus on the top 5 rules. 

```{r}
  inspect(sort(rules_degree_pruned,by='confidence')[1:5])
```

Top 5 rules are all related to Bachelor degree. A high confidence indicates the strong sequential relationship between lhs and rhs, so we can believe that if the we observe the existence of lhs, the rhs is very likely to happen. For example, for the first rule, if a institution have low percentage of first-generation students, low percentage of students applying to pell grant, high median family income, good repayment condition and more students applying to more than 2 school, it's 97% likely to provide Bachelor degree as predomint degree.

We then sort all 195 rules by the lift value. If the lift value equals to 1, it would imply that the probability of the occurrence of lhs and the probability of occurrence of the rhs are independent. In another word, the higher the lift value is, the more likely the lhs and rhs are associated.

```{r}
  inspect(sort(rules_degree_pruned,by='lift')[1:5])
```

Let's take a look at top five rules sorted by lift. These rules all have a lift greater than 3.5, which means that these rules are around 2.5 times more likely to occure than the probability we would expect if lhs and rhs are independet.

Based on the output, some rules with the high confidence value are also included in the rules with the top rule with the highest lift value in the previous section. A rule with a high confidence and lift value indicating strong relationship between lhs and rhs. Basically, all these 5 rules hava a good overall performance.
  
In the following session, we used some visualizations to better understand the total 195 rules. We draw the scatter plot to visualize the relationships between support, confidence and lift.

```{r}
  plot(rules_degree_pruned, method = NULL, measure = "support", shading = "lift",
        interactive = FALSE, data = NULL, control = NULL)
```

  The plot shows a roughly negative relationship between confidence and support. However, the relationship between lift and the other two is not obvious.
  

## 2.7 Conclusions - Association Rules Analysis for Predominant Degree<a id="2.7"></a>

  As a conclusion, all the rules we find in this session are describing the institutions whose predominant degree are either bachelor or certificate. No explicit association is detected between other degree types and the variables. If most students in a school are from a wealthy and  well-educated family, taking more debt for their education,  less likely to apply for Pell grant, the institutions are likely to be bachelor-predominant.

   Moreover, if a institution is a private for-profit school whose students usually have lower debt amount before graduation and lower earnings six years after graduation, the school is more likely to certificate-degree predominant.
  
  
# 3. Cluster Analysis<a id="3"></a>
## 3.1 Goal<a id="3.1"></a>

  By performing Cluster Analysis on College Scoreboard data, we would like to group institutions in such way that schools in the same group are more similiar to each other than to those in other groups. The result of clustering can help us better understand the profiles of insitutions and how they are different from each other.    
     
## 3.2 Data Preparation<a id="3.2"></a>

  In the following analysis, we still used the ```data_clean``` data set in assignment 1 but we did some transformation on this data set to make it appropriate for Clustering Analysis. 
  
  University name, ID, and state columns were not attributes that would be useful in Clustering Analysis since they are either identifier or have too many levels, so we removed these three columns along with a useless index column generated by R when importing data.
  
```{R}
  data_c = read.csv("data_clean.csv",
                   header = TRUE, na.strings = 'NA')
  
  #get rid of ID, university name and state columns, rename the rownames as the university ID. 
  data.with.rownames <- data.frame(data_c[,-c(1:4)], row.names=data_c[,2])
  glimpse(data.with.rownames)
```
  
  The resulting ```data_c``` data set now contained 15 variables with 7793 observations. 
  Since most distance measurements, like Euclidean Distance and Cosine Distance, are only valid for numeric variables, we converted the factor variables into binary dummy variables. 
  
```{R}
  # Create the dummy boolean variables using the model.matrix() function.
  dummy_preddeg = model.matrix(~PREDDEG_factor-1, data.with.rownames)
  dummy_control = model.matrix(~Control_factor-1, data.with.rownames)
```
   
   The ```dummy_preddeg``` and ```dummy_control``` are dummy variables for ```PREDDEG_factor``` and ```Control_factor```, with number of columns equal to number of levels of original variables.
  
  
   In order to make the column name of 8 dummy variables easy to understand, we renamed these variables with the appropriate names and then combined them with the original 13 continuous variables. We also noticed some missing values in this data set and the percentage of missing is not very high, so we decided to remove the records with any missing value. 
   
```{r}
  #rename the coloumn names for dummay variables to make them more readable.
  colnames(dummy_preddeg) <- gsub("PREDDEG_factor","",colnames(dummy_preddeg))
  colnames(dummy_control) <- gsub("Control_factor","",colnames(dummy_control))
  
  #Combine the matrix back with the original dataframe.
  data_combine_c= cbind(data.with.rownames, dummy_preddeg,dummy_control) 
  
  #git rid of the factor coloumns which have been converted to the dummy variable.
  data_ready = data_combine_c[,-c(1:2)]
  
  #remove the missing values
  data_ready %>%
    na.omit(data_ready) %>%
    {.} -> data_clean_c
```
  
  In the data set ```data_clean_c```, all variables are now numeric and appropriate for Clustering Analysis. 
  
   To ensure every variable receving the same weight, we then standardized the numeric variables in the data set. We also noticed the ```Graduate-degree``` only contained zero values, which meant that it couldn't provide any additional information. So we removed this column.
   
```{R}
  #delete the 17 col since its all 0 
  data_final= data.frame(scale(data_clean_c[,-17]))
```
   
   The resulting ```data_final``` is the clean data set for analysis, which has 4528 observations and 20 columns. 
  
  
## 3.3 Objective<a id="3.3"></a>

   As we mentioned before, the goal of this Clustering Analysis is to understand the profile of institutions and see how they are different from each other. More specifically, we have following three objectives:
   
   1. For each cluster, create a concrete profile for insitutions in this clusters.
   
   2. Identify variables that differentiate one cluster from another.
   
   3. See if some characteristics tend to occure together (like high repayment usually comes with low default rate).
   
   In the following analysis, we are going to try three clustering methods: K-means, PAM and Hierarchical. For each method, we will identify the best number of clusters. And then we will compare different methods using clValid to select the best model.
  
## 3.4 Cluster Analysis process - K-means<a id="3.4"></a>

  We set the seed value to 100 so that we will be able reproduce the analysis.
  
```{r}
  set.seed(100)
```
    
  To determine the optimum k value for clustering the observations, we used scree plots to display the ratio of WSS and TSS for 1 cluster to 7 clusters. 
  
```{R}
  # Initialise ratio_ss
  ratio_ss <- rep(0, 7)
  
  # Finish the for-loop
  for (k in 1:7) {
    # Apply k-means to data_final: data_km
     data_km <- kmeans(data_final, k, nstart = 20)
    # Save the ratio between of WSS to TSS in kth element of ratio_ss
    ratio_ss[k] <- data_km$tot.withinss / data_km$totss
  }
  # Make a scree plot with type "b" and xlab "k"
  plot(ratio_ss, type = "b", xlab = "k")
```
    
    From the scree plot, we can tell that 2 clusters or 3 clusters would be worth trying. We choose k=3 for detail analysis here but we will also consider the 2 clusters in the evaluation stage using  ```clValid```. 
  
  
  Then we used k means function with the cluster number of 3, ```nstart =20``` means that we repeat this process 20 times and find the best clustering result. 
  
```{R}
  km_result <- kmeans(data_final, 3, nstart = 20)
  
  data_final%>%
    mutate(cluster = factor(km_result$cluster)) %>%
     {.} -> data_km
```
  
  The corresponding cluster solution is saved in a new column named ```cluster```. The new data set is named as data_km.
  
  To better understand the size of each cluster, we used the dplyr function to produce the corresponding summary statistics.
  
```{R}
    data_km %>%
    group_by(cluster) %>%
       dplyr::summarise(COUNT = n()) 
```
  
  From the output, we see that cluster 1, 2, 3 contain  946, 1440 and 2142 institutions respectively. 
  
## 3.5 Cluster Analysis process - Partitioning around mediods (PAM)<a id="3.5"></a>
  Then we try PAM method.
  
  To determine an optimal number of clusters for Partitioning around mediods, we calculated the silhouette width for cluster numbers from 2 to 10 and plotted a scree plot. 
  
```{r}
  # Calculate silhouette width for many k using PAM
  sil_width <- c(NA)
  
  for(i in 2:10){
    pam_fit <- pam(data_final,diss=FALSE,
                   k = i)
    sil_width[i] <- pam_fit$silinfo$avg.width
  }
  
  plot(1:10, sil_width,
       xlab = "Number of clusters",
       ylab = "Silhouette Width")
  lines(1:10, sil_width)
```
  
  From the scree plot, we can see the 2-cluster solution has the greatest Silhouette Width values 0.32. Still, the Silhouette Width is very low, which suggests the observations were not effectively clustered into different groups.
  
  
## 3.6 Cluster Analysis process - Hierarchical<a id="3.6"></a> 
  The last method we try is Hierarchical Clustering Anlysis.
  
  We used the dist function to calculate the Euclidean distance for our observations and then used hclust function to cluster the observations. 
  
```{r}
  hclust_result = hclust(dist(data_final))
  plot(hclust_result, label=data_final$NPT4_Pulic)
```

A dengogram was also generated to help slect the number of clusters. Due to the large size for our data set, there is not much useful information we could extract from the bottom part of the dengogram. However, there are clearly two very distinct groups at the top of the dengogram. It looks like either two or three groups might be an interesting place to start investigating. 

  
  We try two and three clusters as follows: 
  
```{r}
  data_final %>%
    mutate(hc_cluster2 = factor(cutree(hclust_result,k=2)),hc_cluster3 =factor(cutree(hclust_result,k=3)) ) %>%
     {.} -> data_hc
```
  
  The clustering result for each obeservation was stored a new column ```hc_cluster2``` and ```hc_cluster3```, the updated data set was named with data_hc. Before we dig deeper into the interpretations for cluster solutions, we will use ```clValid``` function in the following section to validate and select the optimum cluster methods and number of clusters. 

  
## 3.7 Cluster Validations and Comparisons<a id="3.7"></a> 
  
To validate different clustering methods and choose number of clusters, we ran a cluster validation using *clValid* function. It allows us to compare different clustering evaluation metrics under different clustering approachs and number of clusters.

```{r}
  #rownames(data_final) = 1:4528
  methods.vec = c("hierarchical","kmeans","pam")
  clValid.result = clValid(data_final,
                           2:5,
                           clMethods=methods.vec,
                           validation="internal",
                           maxitems = 2000000)
  summary(clValid.result)
```

The result shows that the best choice of number of cluster is 2, and the best method is hierarchical. 

  
In the following session, we first analyzed the 2-cluster analysis in details. 

```{r}
  #two cluster solutions 
  data_hc %>%
    group_by(hc_cluster2) %>%
       dplyr::summarise(COUNT = n()) 
```

From the output for the two cluster solutions, the first cluster has 4524 observations while the second cluster has only 4 observations. The unbalance distribution make the clustering result less useful because the profile of second cluster is not general enough  So after exmaine the table produced by ```clValid``` above we decided to use the hierarchical method with 3 cluster solutions as our optimal cluster choice. 
  
  
## 3.8 The Interpretation for 3 cluster hierachical solutions<a id="3.8"></a> 

We use the following codes to generate the histogram for Hierarchical 3 Cluster Solutions.

```{r}
#three cluster solutions 
data_hc %>%
    group_by(hc_cluster3) %>%
       dplyr::summarise(COUNT = n())

data_hc %>%
  group_by(hc_cluster3) %>%
     dplyr::summarise(COUNT = n()) %>%
       ggplot(aes(x = hc_cluster3, y = COUNT)) + geom_bar(stat = "identity") + ggtitle('Histogram for Hierachical 3 Cluster Solutions') + theme(plot.title = element_text(family = "Trebuchet MS", color="#666666", face="bold", size=12, hjust=0.5)) 
```

Based on the output, the first cluster has 2,992 observations, the second cluster has 1,532 observations while the third cluster has 4 observations. 

We first looked the third cluster first to see what make these observations different from others.

```{r}
#select the cluster 3 membership from the original dataset

cluster3 = subset(data_c, UNITID %in% rownames(subset(data_hc,hc_cluster3==3)))[,-1]
cluster3[,1:5]
```
 
 Based on the output, we can tell that all of these four universities are all Private For-profit with the Not-Classified predominant degree. Institutions in this cluster have very high percent of pell grant, very high percentage of loan, slightly high percent of first-generation students compared to other two clusters. 
 
Then We calculated median value of each columns grouped by cluster.

```{r}
#three cluster solution summary statistics 
data_hc[,-21]%>%
  group_by(hc_cluster3) %>%
     dplyr::summarise_each(funs(median)) 
```

We can tell that the cluster 1 are the TITLE IV institutions with low average net price, high percentage first-generation students, low median family income, high percentage of Pell Grant receiving students and federal loan, low median debt, low median income six years after entry and ten years after entry, low repayment rate 3 years after entering repayment. **To summarize, students in these institution tend to from a less wealthy background, need more financial aid, and make less money after graduation**

Insitutions in cluster 2 are opposite to that in cluster1. These insitutions have students from a more educated and wealthy background and they are willing to invest more money in education by seeking student loan or family support. Students also make more money after graduation.

Then we created some visualizations to help us explore the relationships between variables under each cluster. First we plot earnings against percentage of Pell Grant.

```{R}
#ggplot
data_hc %>% 
  ggplot(aes(x=MD_EARN_WNE_P6, y=PCTPELL)) + 
  geom_point(aes(color=hc_cluster3)) + 
  guides(size=FALSE) + ggtitle('Scatter Plot for PCTPELL against MD_EARN_WNE_P under Hierachical 3 cluster solutions ') + theme(plot.title = element_text(family = "Trebuchet MS", color="#666666", face="bold", size=8, hjust=0.5)) +
    xlab("Earnings six years after entry")+
    ylab("Percentage of Pell Grant")
```
  
   From the scatter plot, it is easy to see that the institutions in cluster 1 have a high percentage of students who receive the Pell Grant meanwhile their median earnings 6 years after the enrollment is relative low compared to the records in the cluster 2. 


Likewise, we explored the distributions of earnings for each cluster as below: 

```{r}
data_hc %>%
  ggplot(aes(x = hc_cluster3,
             y = MD_EARN_WNE_P6)) +
  geom_boxplot(aes(fill = hc_cluster3)) +
  xlab("Cluster") +
  ylab("Median Earning six years after entry") +
  scale_fill_discrete(name = 'Cluster') 
```

   It's evident that the the cluster 2 has higher value of earnings than cluster 1 and 3, and about 70% of values in this cluster are above the average earning for all observations. The earning values for cluster 1 and 3 roughly fall in similar range while the cluster 1's observations spread more widely. Moreover, most of the observations in these two clusters are below the average earning value.

We also plotted median family income against earning after six years of entry for each cluster.

```{r}
qplot(data = data_hc,
      x = MD_FAMINC,
      y = MD_EARN_WNE_P6,
      color = hc_cluster3,
      xlab = 'Median Family Income',
      ylab = "Median Earning six years after entry",
      main = "Scatter plot between Median Earning six years after entry and Median Family Income and")
```
  
    We see that the most observations in cluster 2 have higher value for both Median Family Income and Median Earning six years after entry.


## 3.9 Findings and Conclusions<a id="3.9"></a> 

To find the intrinsic grouping in the College Scorecard data set, we used K-means, PAM and Hierarchical methods to perform clustering analysis. The resulting clusters from each method were different so we then used metrics such as Connectivity, Dunn Index and Silhouette width to evaluate how well observations were clustered. 

The optimal method and number of clusters we found was Hierarchical method with three clusters. When selecting the best methods and number of clusters to use, we took into account not only the clusters performance indicated by evaluation metrics but also human judgement on the usefulness of the clustering result. Therefore, we decided to give up the best result generated by computer - Hierarchical with 2 clusters and go with Hierarchical with 3 clusters.

The final three clusters contain 2992 observations, 1532 observations and 4 observations respectively. It's unusual that one cluster only contains 4 observation which may not be general enough to give us some useful insights. In the future, one alternative study we will conduct is to treat these 4 observations as outliers and do another cluster analysis without these 4 rows to if the result is different.

The result shows that institutions from cluster 1 tend to have students from less wealthier background and need more pell grant for their education. These students also make less money after graduation. In contrast, cluster 2 contains institutions whose students come from more educated and wealthier family, and make more money after graduation. 


# 4.Further Studies<a id="4"></a>

The association rules we have extracted from this stage can be used to determine the candidate predictors for predicting colleges' post-graduation earnings in the future stages. Likewise, clustering result can also be used as a new input in the predictive model.

In terms of the solution we have selected for the cluster analysis, the results might be improved by taking a further investigation on the 4 observations in cluster 3. The analysis process covered in this paper could be revisited if the 4 observations in the cluster 3 are outliers, thus can be removed. Moreover, a density-based cluster method like DBSCAN can be included in the analysis process as well.




