---
title: "MA710-A2- College Scorecard Association Rules & Cluster Analysis"
author: "Xiang Li, Xue Zhou"
date: "March 11, 2017"
output: html_document
---

#Table of Contents
* 1.[Introduction](#Introduction)
* 2.[Association Rule Analysis]
  * 2.1 [Goal]
  * 2.2 [Data Preparation]
  * 2.3 [Objective]
  * 2.4 [Association Rule Analysis - Median earning six years after entry]
  * 2.5 [Association Rule Analysis - Predominant Degree]
  * 2.6 [Findings and Conclusions]
* 3.[Cluster Analysis]
  * 3.1 [Goal]
  * 3.2 [Data Preparation]
  * 3.3 [Objective]
  * 3.4 [K-menas Cluster Analysis ]
  * 3.5 [Hierarchical clustering Analysis ]
  * 3.6 [Comparision]  
  * 3.7 [Findings and Conclusions]
* 4[Further Studies]
       


```{r setup, include=FALSE,warning=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message = FALSE)
```

#1. Introduction
    In this stage, we will some unsupervised learning techniques to analyze the College Scorecard Data. 
    The main techniques we use are association rule learning and cluster analysis.
    
#2. Association Rule Analysis
## 2.1 Goal 
## 2.2 Data Preparation 
 First, we load all the packages required for analysis.
```{r}
library(readr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(ggvis)
library(plyr)
library(arules)
library(arulesViz)
```

Then import the dataset
```{r}
data = read.csv("data_clean.csv",
                 header = TRUE, na.strings = 'NA')
names(data)
```

Since first three columns are not useful in the following association rule analysis,
we took out these three columns and then remove all the records with any missing values.
Rename the variable to make the more readable.The output returned is clean data set we can use in the following analysis.

```{r}
data_asso = data[,-c(1,2,3,17)]
#data_asso = na.omit(data_asso)
data_asso %>% dplyr::rename(state = STABBR_factor,
                     pred_degree = PREDDEG_factor,
                     control = Control_factor,
                     net_cost = NPT4_COMBINE,
                     per_independent = DEP_STAT_PCT_IND,
                     per_1generation = PAR_ED_PCT_1STGEN,
                     median_family_inc =MD_FAMINC,
                     per_pell = PCTPELL,
                     per_loan = PCTFLOAN,
                     debt_grad = GRAD_DEBT_MDN,
                     debt_non_grad = WDRAW_DEBT_MDN,
                     per_app_greater2 = APPL_SCH_PCT_GE2,
                     median_earning_6years = MD_EARN_WNE_P6,
                     repayment_rate = RPY_3YR_RT_SUPP,
                     default_rate = CDR3) %>%
                     {.} -> data_asso

```
The next step would be to encode all numeric variables into categorical variables based on their quantiles. To this end, we first create a function *make.ntiles* which allows us to encode numerical variables into categorical variables of n level.

```{r}
# create factor variables from all of the numeric variables you chose to work with using the make.ntiles function.

make.ntiles = function (inputvar, n) {
  inputvar %>%
quantile(.,
(1/n) * 1:(n-1),
             na.rm=TRUE
    ) %>%
c(-Inf, ., Inf) %>% cut(inputvar,
breaks=.,
paste("Q", 1:n, sep="") )
}
```


<!-- #divide the wage into 4  ranges.  -->
<!-- data %>%  -->
<!--   mutate(MD_EARN_WNE_P6.F=make.ntiles(MD_EARN_WNE_P6, 4), -->
<!--          NPT4_COMBINE.F = make.ntiles(NPT4_COMBINE, 4), -->
<!--          DEP_STAT_PCT_IND.F = make.ntiles(DEP_STAT_PCT_IND, 4), -->
<!--          PAR_ED_PCT_1STGEN.F = make.ntiles(PAR_ED_PCT_1STGEN, 4), -->
<!--          MD_FAMINC.F = make.ntiles(MD_FAMINC, 4), -->
<!--         PCTPELL.F = make.ntiles(PCTPELL, 4), -->
<!--          PCTFLOAN.F = make.ntiles(PCTFLOAN, 4), -->
<!--          MD_FAMINC.F = make.ntiles(MD_FAMINC, 4), -->
<!--         GRAD_DEBT_MDN.F = make.ntiles(GRAD_DEBT_MDN, 4), -->
<!--          WDRAW_DEBT_MDN.F = make.ntiles(WDRAW_DEBT_MDN, 4), -->
<!--          APPL_SCH_PCT_GE2.F = make.ntiles(APPL_SCH_PCT_GE2, 4), -->
<!--         RPY_3YR_RT_SUPP.F = make.ntiles(RPY_3YR_RT_SUPP, 4), -->
<!--         CDR3.F = make.ntiles(CDR3, 4)) %>%  -->
<!--     {.} -> data_new -->
<!-- names(data_new) -->
<!-- summary(data_new) -->

<!-- data_final = data_new[-c(1:2,5:16)] -->
<!-- names(data_final) -->

<!-- rules = apriori(data_final) -->
<!-- inspect(rules) -->


After creating the function make.ntiles, we apply the make.ntiles on every numeric column.The resulting output would be a set of variables encoded from numeric variables according to their quantiles.Then we combine the encoded variables with the original three categorical variables in the dataset to form the full dataset.


```{r}
data_asso %>% 
  sapply(.,is.numeric) %>%
  data_asso[,.] %>%
  apply(.,make.ntiles, n=5, MARGIN = 2) %>%
  {.} -> data_num
data_clean = cbind(data_num,data_asso[,c(1,2,3)])
```


## 2.3 Objective 
    In this session, we would like to leverage a descripitve analysis tool - association rule to explore relationship between post-graduation earning, predominant degree and other variables. For the purpose of consistency, we continue to use selected variables in the assignment one. Our two objectives for this part of analysis are: identifying factors that have notable relationship with post-earning so that they can be used later to predict and influence eaning, describing the profile of university awarding different types degree with related attributed.

## 2.4 Association Rule Analysis - Median earning six years after entry
In the following part, we are going to conducting assication rule analysis with right-hand side set to median income six years after entry.

```{r}
rules = apriori(data_clean)
inspect(rules)
```

```{r}
#filter the rhs to the MD_EARN_WNE_P6.F column.
rules_subset <- subset(rules,
(rhs %in% c("median_earning_6years=Q1",
"median_earning_6years=Q2",
"median_earning_6years=Q3",
"median_earning_6years=Q4"))
)

```

```{r}
#sort by support, lift and confidence
rules_support = sort(rules,by="support", decreasing = T)
rules_support = sort(rules,by="confidence", decreasing = T)
rules_support = sort(rules,by="lift", decreasing = T)



```



## 2.5 Association Rule Analysis - Predominant Degree

At the beginning of our association rule analysis, we set some parameters that can generate desirable output for use.The right-hand side is set to predominant degree, which indicates we only study what factors may be more likely to exist with different degree types simultaneously. 
```{r}
apriori.appearance = list(rhs = c("pred_degree=Bachelor's-degree","pred_degree=Certificate-degree",
                                  "pred_degree=Associate's-degree","pred_degree=NotClassified",
                                  "pred_degree=Graduate-degree"),
                          default = 'lhs')
```

Then we set the minimum support to 0.1, which means that only combination existing more than a frequency of 10% will be chosen.Likewise, confidence is set to 0.5, indicating that only rules with conditional probability greater than 0.5 will be chosen.

```{r}
apriori.parameter = list(support = 0.1,
                          confidence =0.5)
apriori.control = list(verbose = FALSE)
rules = apriori(data_clean,
                parameter = apriori.parameter,
                appearance = apriori.appearance,
                control = apriori.control)
length(rules)
```
In total, we get 108 association rules.

```{r}
inspect(sort(rules,by='lift')[1:5])
inspect(sort(rules,by='confidence')[1:5])
inspect(sort(rules,by='support')[1:5])
```
If we take a look at first five association rules ordered by lift, we notice that the right-hand side of these five rules are all bachelor degree, and they all have relatively low support. Plot the support versus lift of all the rules.
```{r}
plot(rules, method = NULL, measure = "support", shading = "lift",
      interactive = FALSE, data = NULL, control = NULL)
```

Visuallly represented

```{r}
subrules <- rules[quality(rules)$confidence > 0.8 & quality(rules)$support >0.15]
subrules2 <- head(sort(rules, by="lift"), 5)
plot(subrules2,method="graph", control=list(type="itemsets"))
```

show rules for bachelor degree
```{r}
rules_bachelor <- subset(rules, (rhs %in% c("pred_degree=Bachelor's-degree")))
inspect(sort(rules_bachelor,by='lift')[1:5])
plot(sort(rules_bachelor,by='lift')[1:5],method="graph", control=list(type="itemsets"))
```

There are 80 rules with pred_degree=Bachelor's-degree as their right-hand side, which indicates that more significant relationship exist between bachelor degree are other variables.Taking the first rule as an example, schools with students whose family has a high income,low percentage of students applying for pell-grant, and low default rate are more likely to be classified as bachelor degree predomint school. This imply a positive relationship between the level of education and students' wealth situation-students in school offering high degree are more likely to come from wealthier family.

Show rules for certificate degree.
```{r}

rules_certificate <- subset(rules, (rhs %in% c("pred_degree=Certificate-degree")))
inspect(sort(rules_certificate,by='lift')[1:5])
plot(sort(rules_certificate,by='lift')[1:5],method="graph", control=list(type="itemsets"))
```
There are 25 rules for certificate degree. Also taking the first rule as an example, institutions with attending students whose debt are low, and have a control type equal to private for-profit are morely to classified as certificate-degree predominant. Looking at the first five rules, we also notice that these institutions usually have graduation with low post-graduation earning, and low debt after completion

Show rules for associate degree
```{r}
rules_associate <- subset(rules, (rhs %in% c("pred_degree=Associate's-degree")))
inspect(sort(rules_associate,by='lift'))
#There are only three rules for associate degree type. Let's look at all of them. Association rules show that net cost for associate-degree predominant school are usually low, and they tend to belong to public type and low percentage of students applying for loan.


```


Show rules for master degree
```{r}
rules_master <- subset(rules, (rhs %in% c("pred_degree=Graduate-degree")))
inspect(sort(rules_master,by='lift'))
```

Show rules for Not-Classified
```{r}
rules_notclassified <- subset(rules, (rhs %in% c("pred_degree=NotClassified")))
inspect(sort(rules_notclassified,by='lift'))
```






## 2.6 Conclusions



#3. Cluster Analysis 
## 3.1 Goal
   The goal of this chapter is to find natural groups of similar institutions and to describe or characterize the institutions in these groups. 

## 3.2 Data Preparation


```{R}
data = read.csv("data_clean.csv",
                 header = TRUE, na.strings = 'NA')

#get rid of ID, university name and state columns, rename the rownames as the university ID. 
data.with.rownames <- data.frame(data[,-c(1:4)], row.names=data[,2])
names(data.with.rownames)
glimpse(data.with.rownames)
```

In order for a yet-to-be-chosen algorithm to group observations together, we first need to define some notion of (dis)similarity between observations. Considering that most popular choice for clustering like Euclidean distance is only valid for continuous variables, we need to convert the factor variables into binary dummy variables. 

```{R}
# Create the dummy boolean variables using the model.matrix() function.
dummy_preddeg = model.matrix(~PREDDEG_factor-1, data.with.rownames)
dummy_control = model.matrix(~Control_factor-1, data.with.rownames)
dummy_preddeg 
dummy_control

#rename the coloumn names for dummay variables to make them more readable. 
colnames(dummy_preddeg ) <- gsub("PREDDEG_factor","",colnames(dummy_preddeg))
colnames(dummy_control  ) <- gsub("Control_factor","",colnames(dummy_control))
dummy_preddeg 
dummy_control

#Combine the matrix back with the original dataframe.
data_combine= cbind(data.with.rownames, dummy_preddeg,dummy_control) 

#git rid of the factor coloumns which have been converted to the dummy variable.
data_ready = data_combine[,-c(1:2)]
data_ready
glimpse(data_ready)

#remove the missing values
data_ready %>%
  na.omit(data_ready) %>%
  {.} -> data_clean
data_clean
```

All the columns in the dataset data_ready are continuous variables, which are applicable for calculating the (dis)similarity between observations. To ensure the clustering results are able to reveal the valuable information to maximum, we decide to standardize every numeric variable in the dataset. 

```{R}
data_num = data.frame(scale(data_clean[,-c(14:21)]))
data_final = cbind(data_num, data_clean[,c(14:21)])

```
The data_final will be used as our final dataset to conduct further clustering analysis. 

## 3.3 Objective
## 3.4 K-menas cluster analysis 

```{R}
#set random seed to reproduce the results
set.seed(100)
# Initialise ratio_ss
ratio_ss <- rep(0, 7)

# Finish the for-loop
for (k in 1:7) {
  
  # Apply k-means to data_final: data_km
   data_km <- kmeans(data_final, k, nstart = 20)
  
  # Save the ratio between of WSS to TSS in kth element of ratio_ss
  ratio_ss[k] <- data_km$tot.withinss / data_km$totss
  
}

# Make a scree plot with type "b" and xlab "k"
plot(ratio_ss, type = "b", xlab = "k")


## choose K, find k that minimizes WSS: scree plot
##tot.withiss   : wss
## betweenss: bss


```

from the scree plot, we can tell that the plot shows a considerable drop for k equal to 3. We will choose k=3 as out optimal k value. 

```{R}
# Cluster the data using k-means: data_km2. 3 clusters, repeat 20 times

data_km2 <- kmeans(data_final, 3, nstart = 20)
names(data_final)

#interpretation with descriptive statistics: the summary data for the 3 resulting clusters

k_results <- data_final %>%
  mutate(cluster = data_km2$cluster) %>%
  group_by(cluster) %>%
  do(the_summary = summary(.)) %>%
k_results$the_summary



# Plot the observations with Color using clusters
plot(data_final$MD_EARN_WNE_P6, data_final$PCTPELL, col = data_km2$cluster,
     xlab = "MD_EARN_WNE_P6", ylab ="PCTPELL", main = "MD_EARN_WNE_P6")
 
##ggplot
library(ggplot2)
ggplot(data=data_final, aes(x=MD_EARN_WNE_P6, y=PCTPELL, color=factor(data_km2$cluster ))) + geom_point() 



```

## 3.5 Hierarchical Clustering Analysis 


## 3.6 Comparision 
## 3.7 Conlusions 


