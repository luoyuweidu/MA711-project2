---
title: "MA710-A2- College Scorecard Association Rules & Cluster Analysis"
author: "Xiang Li, Xue Zhou"
date: "March 11, 2017"
output: html_document
---

#Table of Contents
* 1.[Introduction](#Introduction)
* 2.[Association Rule Analysis]
  * 2.1 [Goal]
  * 2.2 [Data Preparation]
  * 2.3 [Objective]
  * 2.4 [Association Rule Analysis - Median earning six years after entry]
  * 2.5 [Association Rule Analysis - Predominant Degree]
  * 2.6 [Findings and Conclusions]
* 3.[Cluster Analysis]
  * 3.1 [Goal]
  * 3.2 [Data Preparation]
  * 3.3 [Objective]
  * 3.4 [Cluster Analysis -K means]
  * 3.5 [Cluster Analysis -PAM]
  * 3.6 [Cluster Analysis -DBSCAN]
  * 3.7 [Cluster Analysis -Hierarchical]
  * 3.8 [Cluster Validation and Comparison]
  * 3.9 [Findings and Conclusions]
* 4[Further Studies]
       


```{r setup, include=FALSE,warning=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message = FALSE)
```

#1. Introduction
    In this stage, we will use some unsupervised learning techniques to analyze the College Scorecard Data. The main techniques we use are association rule mining and cluster analysis.
    
#2. Association Rule Analysis
## 2.1 Goal 
    At the first stage(Assignment1), we've explored the relationship between pairs of variables selected from the full College Scorecard dataset. Now we proceed to study two variables that most interested us: the students' post-graduation earnings and the institutions' predominant degree types. We will use association rule mining technique to explore relationship between post-graduation earnings, predominant degree and other variables. We used the clean dataset(data_clean.csv) in the Assignment 1 as our original dataset in this section.
    

## 2.2 Data Preparation 
First, we loaded all the packages required for analysis as below: 
```{r}
library(readr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(ggvis)
library(plyr)
library(arules)
library(arulesViz)
library(cluster)
library(clValid)
```

Then we imported the dataset *data_clean*. 
```{r}
data = read.csv("data_clean.csv",
                 header = TRUE, na.strings = 'NA')
```

Considering the column X, UNITID, INSTNM_factor and MD_EARN_WNE_P10 are not related to our analysis at this stage, we removed these four variables from the dataset. Then we renamed the column names to make them easier to understand. 
```{r}
data_asso = data[,-c(1,2,3,17)]
#data_asso = na.omit(data_asso)
data_asso %>% dplyr::rename(state = STABBR_factor,
                     pred_degree = PREDDEG_factor,
                     control = Control_factor,
                     net_cost = NPT4_COMBINE,
                     per_independent = DEP_STAT_PCT_IND,
                     per_1generation = PAR_ED_PCT_1STGEN,
                     median_family_inc =MD_FAMINC,
                     per_pell = PCTPELL,
                     per_loan = PCTFLOAN,
                     debt_grad = GRAD_DEBT_MDN,
                     debt_non_grad = WDRAW_DEBT_MDN,
                     per_app_greater2 = APPL_SCH_PCT_GE2,
                     median_earning_6years = MD_EARN_WNE_P6,
                     repayment_rate = RPY_3YR_RT_SUPP,
                     default_rate = CDR3) %>%
                     {.} -> data_asso

```
The *data_asso* is the resulting data set. 

To make the dataset applicable for association rules mining, we need to encode all the numeric variables into categorical variables with appropriate levels. We use the following function *make.ntiles* to perform this conversion.  
```{r}
# create factor variables from all of the numeric variables you chose to work with using the make.ntiles function.

make.ntiles = function (inputvar, n) {
  inputvar %>%
quantile(.,
(1/n) * 1:(n-1),
             na.rm=TRUE
    ) %>%
c(-Inf, ., Inf) %>% cut(inputvar,
breaks=.,
paste("Q", 1:n, sep="") )
}
```
With the above function, we are able to convert each numeric variable into a factor variable with n levels. In our case, we decided to encode all the numeric variables into 3 categories, thus n =3. 


Then we applied the make.ntiles function on every numeric column. The resulting output would be a set of variables encoded from numeric variables according to their quantiles. The Q1 is represtantive of the range for low value, the Q2 is represtantive of the range for medium value and the Q3 is represtantive of the range for high value. Then we combined the encoded variables with the original three categorical variables in the dataset to generate a complete dataset. 
```{r}
data_asso %>% 
  sapply(.,is.numeric) %>%
    data_asso[,.] %>%
      apply(.,make.ntiles, n=3, MARGIN = 2) %>%
         as.data.frame()   %>%
             {.} -> data_num
data_clean = cbind(data_num,data_asso[,c(1,2,3)])
```
As a result, the dataset *data_clean* would be the prepared dataset we would use in the further association rule mining. 


## 2.3 Objective 
Specifically, our two objectives for this part of analysis are: 
      1. To identify factors that have notable relationship with post-graduation earnings so that they can be used later to predict the post-graduate earnings. 
      2. To describe the profile of university awarding different types degree using related attributes. 
    
## 2.4 Association Rule Analysis - Median earning six years after entry
We used the following code to generate the association rules for the *data_clean* dataset. We used the parameter, appearance and control parameters of the apriori command below to set the requirements for the rules it will generate. In this case, the appearance parameter indicates that the consequent (right hand side) of all generated rules should be one of the three values of the median_earning_6years variable. The parameter parameter indicates that all rules generated by this command must have support greater than 0.05 and confidence greater than 0.8. The minlen and maxlen parameters specify that all generated rules with a length between 2 to 4. 
```{r}
#filter the rhs to the MD_EARN_WNE_P6.F column.
apriori.appearance_e = list(rhs=c('median_earning_6years=Q1','median_earning_6years=Q2','median_earning_6years=Q3'), default='lhs')
apriori.parameter_e = list(support=0.05,
                         confidence=0.8,minlen=2, maxlen=4)
apriori.control_e = list(verbose=FALSE)
rules_earning = apriori(data_clean,   
                 parameter=apriori.parameter_e,
                 appearance=apriori.appearance_e,
                 control=apriori.control_e)
length(rules_earning)
```
In total, there are 95 rules generated based on the parameters we specified before. 

A rule is redundant if a more general rules with the same or a higher predictive power exists. Considering the redundant rules provide no extra information in addition to the valid rules, we only kept the rules which were not redundant. We will use *lift* measure to determine the redundant rules. The frequency for each item was also analyzed in this session.
```{r}
rules.sorted <- sort(rules_earning, by="lift") 
length(rules.sorted)

## redundant rules
rules_redundant = rules.sorted[is.redundant(rules.sorted)]
rules_redundant 

## non-redundant rules
rules_pruned = rules.sorted[!is.redundant(rules.sorted)] 
rules_pruned 

##The number of rules associated with rhs of median_earning_6years=Q2
rules_Q2 = subset(rules_pruned, subset = items %in% "median_earning_6years=Q2")
length(rules_Q2)


##The number of rules associated with rhs of median_earning_6years=Q3
rules_Q3 = subset(rules_pruned, subset = items  %in% "median_earning_6years=Q3")
length(rules_Q3)

#Item frequency PLot
itemFrequencyPlot(items(rules_pruned), topN=25, cex.names=.6)


```
From the output, there are 8 redundant rules in this case. Thus we would only focus on the remaining 87 rules. Among the 87 valid rules, 84 of them have the rhs of median_earning_6years=Q3 while the 3 of them have the rhs of median_earning_6years=Q2, no rules were found to contain the rhs of  median_earning_6years=Q1. Based on the Item Frequency graph, it is also obvious that the item *median_earning_6_years = Q3* has the highest frequency which is more than 80%.


Next we sorted the resulting rules by *support* to take a further analysis and we specifically focused on the top 5 rules. A balloon graph visualizing these 5 rules is also displayed. 
```{r}
#sort by support 
rules_support = sort(rules_pruned,by="support", decreasing = T)
inspect(rules_support[1:5])


plot(head(sort(rules_pruned, by="support"), 5),
  method="grouped")



```
Based on the output, the rules with the highest support value all have RHS(right hand side) as median_earning_6years=Q3. If we take the first rule as an example. The support value of 0.1256 means that the frequency of the itemset in this rule is 12.56% - in the dataset, 12.56% of the universities have high median family income, high debt amount before graduation, high repayment rate and high median earnings 6 years after they were enrolled. The confidence value is 0.8091, indicating that 80.91% of the universities whose students have high median family income, high debt amount before graduation and high repayment rate are the universities whose students have high median earning 6 years after the enrollment.The lift value of 3.23 means that universities qualified for both antecedent and consequent conditions are 223% higher than we would expect if antecedent and consequent are independent. The high value of confidence and lift indicates that the rule has a good performance.  


Then we ordered the pruned rules by confidence and pulled out the top 5 rules. A balloon graph visualizing these 5 rules is also displayed. 
```{r}
rules_confidence = sort(rules_pruned,by="confidence", decreasing = T)
inspect(rules_confidence[1:5])

plot(head(sort(rules_pruned, by="confidence"), 5),
  method="grouped")

```
A high value of confidence means that the frequency of the if/then statement found to be true is high. From the balloon graph, we can tell that the 5 resulting rules have relative high support value and the rule with item per_pell=Q1 has the highest lift value. If we take the first rule as an example, the support value of 0.9094 means that 90.94% of the universities who have a low percentage in Pell grant, high value in debt_non_grad and low value in default rate are the universities with a high median earning value. The high value of lift(3.63) also indicates that the relationship between the antecedent(per_pell=Q1,debt_non_grad=Q3, default_rate=Q1) and the consequent(median_earning_6years=Q3) is much more significant than would be expected if the two sets were independent. 


Similarly, we sorted the rules by lift value and displayed the top 5 rules. 
```{r}
rules_lift = sort(rules_earning,by="lift", decreasing = T)
inspect(rules_lift[1:5])

```
From the output, all the five rules have the lift value large than 1, indicating that all five rules are useful in finding the consequent item sets. Moreover, the rule with the highest lift value is identical with the rule with the highest confidence value in the previous session , which means that there exists a notable relationship between antecedent items(low Pell percentage value, high debt before graduation and low default rate value) and consequent item(high median post-graduation earnings after enrollment) within this rule. 

###2.4.4 Conclusion 
Based on the association rule mining so far, we are able to identify some potential factors affecting students' post-graduation earnings in an institution: The instituions with high post-graduation earnings of students are more likely to exist among the schools which has a low percemtage of Pell Grant students, low default rate, high repayment rate and whose students have a well-educated family background with high family income. 


## 2.5 Association Rule Analysis - Predominant Degree
The second part of association rule analysis is mainly focused on the relationship between predominant degree type and the other variables. At the beginning of our association rule analysis, we set some parameters that can generate rules of interest. As the following code indicates, the right-hand side is limited to the predominant degree. 
```{r}
apriori.appearance = list(rhs = c("pred_degree=Bachelor's-degree","pred_degree=Certificate-degree",
                                  "pred_degree=Associate's-degree","pred_degree=NotClassified",
                                  "pred_degree=Graduate-degree"),
                          default = 'lhs')
```


Then we set the minimum support to 0.1, which means that the algorithm will only keep the rules with a frequency greater than 10%. Likewise, the minimum confidence is set to 0.8, which means that we will only keep the rules with the conditional probability greater than 0.8. 
```{r}
apriori.parameter = list(support = 0.10,
                          confidence =0.8)
apriori.control = list(verbose = FALSE)
rules_degree = apriori(data_clean,
                parameter = apriori.parameter,
                appearance = apriori.appearance,
                control = apriori.control)
length(rules_degree)
```
In total, we get 225 association rules. Like what we did in the 2.2 session, we also removed redundant rules as the code block displayed below. 
```{r}

rules_degree_sorted <- sort(rules_degree, by="lift") 
length(rules_degree_sorted)

## redundant rules
rules_degree_redundant = rules_degree_sorted[is.redundant(rules_degree_sorted)]

## non-redundant rules
rules_degree_pruned = rules_degree_sorted[!is.redundant(rules_degree_sorted)] 

rules_degree_redundant 
rules_degree_pruned
```
After removing the redundant rules, we got 195 rules in total. We are going to study these rules in the following part.


We first sorted all 195 rules by the lift value. Lift indicates the ratio of the observed support to that expected if two items were independent. If the lift equals to 1, it would imply that the probability of occurrence of antecedent and that of the consequent are independent of each other. With that said, the higher the lift is, the more likely the lhs and rhs are associatied.
```{r}
inspect(sort(rules_degree_pruned,by='lift')[1:5])
```
Let's take first rule as an example. A school with low percentage of first generation students, high family income, low percentage of pell grant, high percentage of student applying to more than one school, high percentage of repayment rate is more likely to be a bachelor-degree predominant institution. Specifically, the value of support 0.102 means that about 10% of schools satisfy the both sides above. A confidence value 0.97 indicates that among all schools with attributes at left-handed side, 97% would offer bachelor degree as their predominant degree. A lift as high as 3.5 shows that the probability of occurrence of this rule is 3.5 times the expected probability if left-hand side and right-hand side are independent.   


Then we look at the rules sorted by confidence.
```{r}
inspect(sort(rules_degree_pruned,by='confidence')[1:5])
```
We realized that the top rule with the highest confidence value is also the top rule with the highest  lift value, which indicates that this rule has a good overrall performance. 


Lastly, we look at rules sorted by support and focused on the top 5 rules. 
```{r}
inspect(sort(rules_degree_pruned,by='support')[1:5])
```
This time we got a different rule at the top place. A support 0.19 means that about 20% of schools are private for-profit, with predominant degree type Certificate and whose enrolling students have low debt amount. 


Then we used some visualizations to better understand these rules. We drawed the scatter plot to visulize the relationship among support, confidence and lift. 
```{r}
plot(rules_degree_pruned, method = NULL, measure = "support", shading = "lift",
      interactive = FALSE, data = NULL, control = NULL)
```
The plot shows a negative relationship between confidence and support. However, the relationship between lift and the other two is not obvious.


In order to profile the instituions under each predominant degree type, we segmented the association rules by their right-hand side and then mainly focused on the top 5 rules from the resulting rules. 
```{r}
rules_bachelor <- subset(rules_degree_pruned, (rhs %in% c("pred_degree=Bachelor's-degree")))
rules_bachelor
inspect(sort(rules_bachelor,by='lift')[1:5])
plot(sort(rules_bachelor,by='lift')[1:5],method="graph", control=list(type="itemsets"))
```
Based on the output, 185 of the total 195 rules have right-hand side pred_degree=Bachelor's-degree, which indicates that most relationships among the total 195 rules are between bachelor degree and other variables.After sorting the rules by lift, we were able to further analyze the top1 rule. Obviously, the schools with students whose family has a high income,low percentage of students applying for pell-grant and low default rate are the schools whose predominant degree is bachelor degree. 

Blow is the code to filter the rules whose rhs contain item pred_degree=Certificate-degree. 
```{r}
rules_certificate <- subset(rules_degree_pruned, (rhs %in% c("pred_degree=Certificate-degree")))
rules_certificate
inspect(sort(rules_certificate,by='lift')[1:5])
plot(sort(rules_certificate,by='lift')[1:5],method="graph", control=list(type="itemsets"))
```
There are 10 resulting rules in the output. Taking the first rule as an example, we can see the private for-profit institutions with attending students whose debt are low are more likely to be certificate-degree predominant. Looking at the first five rules, we also notice that these institutions usually have graduats with low post-graduation earnings, and low debt after completion.


## 2.6 Conclusions
To conclude, all the rules we found in this session are discribing the institutions whose predominant degree are either bachelor or certificate. No explicit association is detected between other degree type and the variables in the data set. In institutions whose predominant degree are bachelor, the students are more likely to from wealthy family with well-educated family, willing to take more debt for their education, and are less likely to apply for Pell grant. 

Moreover, the certificate-predominant univerisities are more likely to be private for-profit schools. Students in these schools usually have lower debt amount before graduation and lower earnings six years after graduation compared to other types of schools. 

With the above information, we are able to identify some potential attributes, which can be used to profile these two types of schools: family income, family education background, Percentage of Pell Grant, control type, debt amount before graduation and repayment rate. 

#3. Cluster Analysis 
## 3.1 Goal
The goal of this chapter is to find natural groups of similar institutions and to characterize the institutions in these groups. 
   

## 3.2 Data Preparation
Similar to the last section, we used the data_clean in assignment 1 as our original dataset. Then we deleted the first four columns and renamed the row names with the university ID. 
```{R}
data_c = read.csv("data_clean.csv",
                 header = TRUE, na.strings = 'NA')

#get rid of ID, university name and state columns, rename the rownames as the university ID. 
data.with.rownames <- data.frame(data_c[,-c(1:4)], row.names=data_c[,2])
names(data.with.rownames)
glimpse(data.with.rownames)
```
The data_c dataset now contained 15 variables with 7793 observations. 


Considering that most popular choices for clustering method like Euclidean distance is only valid for continuous variables, we need to convert the factor variables into binary dummy variables. 
```{R}
# Create the dummy boolean variables using the model.matrix() function.
dummy_preddeg = model.matrix(~PREDDEG_factor-1, data.with.rownames)
dummy_control = model.matrix(~Control_factor-1, data.with.rownames)
```


To make the converted dummy variables easier to understand, we renamed the 7 new columns with the appropriate names and combined them with the original 13 continuous variables. Considering the dataset contained certain amount of missing values, we removed the records with any missing cells. 
```{r}
#rename the coloumn names for dummay variables to make them more readable. 
colnames(dummy_preddeg ) <- gsub("PREDDEG_factor","",colnames(dummy_preddeg))
colnames(dummy_control  ) <- gsub("Control_factor","",colnames(dummy_control))

#Combine the matrix back with the original dataframe.
data_combine_c= cbind(data.with.rownames, dummy_preddeg,dummy_control) 

#git rid of the factor coloumns which have been converted to the dummy variable.
data_ready = data_combine_c[,-c(1:2)]

#remove the missing values
data_ready %>%
  na.omit(data_ready) %>%
  {.} -> data_clean_c
data_clean_c
```
The clean dataset was stored as *data_clean_c*. All the columns in the dataset data_c are continuous variables, which are applicable for calculating the (dis)similarity between observations. 


To ensure the clustering analysis gives each variable the same weight, we decide to standardize every numeric variable in the dataset. Also, the column 17 only contains 0, which can not provide any information for us, so we decided to remove this column.
```{R}
#delete the 17 col since its all 0 
data_final= data.frame(scale(data_clean_c[,-17]))
```
The *data_final* will be used as our final dataset to conduct further clustering analysis. The dataset *data_final* has 4528 observations and 20 columns in total. 


## 3.3 Objective
Our objective for cluster analysis is to group instituions together into certein clusters that share similar characteristics as determined by several measures of association. Specifically, we will take into account the following cluster analysis approches: K-means, Partitioning Around Medoids and Hierarchical agglomerative. Based on the existing candidate resulting clusters, we will use clValis function to validate the best cluster group. After the best clusters are finalized, we will conduct conduct descriptive analysis to describe the clusters of interest and use 2-dimensional scatter plot to visualize the associations between pairs of variables from the perspective of cluster assignemnt.

## 3.4 Cluster Analysis process - K-menas cluster analysis 
To reproduce the results, we set the seed value to 100 as the code below.
```{r}
set.seed(100)
```


To determine the optimum k value for clustering the observations, we drawed the scree plots to display the ratio of WSS and TSS and the solutions from 1 cluster to 7 clusters. 
```{R}
# Initialise ratio_ss
ratio_ss <- rep(0, 7)

# Finish the for-loop
for (k in 1:7) {
  # Apply k-means to data_final: data_km
   data_km <- kmeans(data_final, k, nstart = 20)
  # Save the ratio between of WSS to TSS in kth element of ratio_ss
  ratio_ss[k] <- data_km$tot.withinss / data_km$totss
}

# Make a scree plot with type "b" and xlab "k"
plot(ratio_ss, type = "b", xlab = "k")
```
from the scree plot, we can tell that the plot shows a considerable drop for k equals to 3. We will choose k=3 as out optimal k value. 

Then we use k means function with the cluster number of 3, repeating 20 times. 
```{R}
km_result <- kmeans(data_final, 3, nstart = 20)

data_final%>%
  mutate(cluster = factor(km_result$cluster)) %>%
   {.} -> data_km

#interpretation with descriptive statistics: the summary data for the 3 resulting clusters
km_summary = data_km %>%
  group_by(cluster) %>%
    do(the_summary = summary(.)) 

km_summary$the_summary
```
The corresponding cluster solution is saved in a new column cluster. The new dataset is named with data_km.


Then we created some visualizations to help us better understand each cluster. We first plotted median earning after six years against percentage of pell grant for each cluster.  
```{r}
# Plot the observations with Color using clusters
data_km %>% 
  ggplot(aes(x=MD_EARN_WNE_P6, y=PCTPELL)) + 
  geom_point(aes(color=cluster)) + 
  guides(size=FALSE)

```


Then we look at the earning distribution for each cluster by creating a box plot. 
```{r}
names(data_km)
data_km %>%
  ggplot(aes(x = cluster,
             y = MD_EARN_WNE_P6)) +
  geom_boxplot(aes(fill = cluster)) +
  xlab("Cluster") +
  ylab("Median Earning six years after entry") +
  scale_fill_discrete(name = 'Cluster')
```
It seems that institutions in cluster 1 have student with higher post-graduation earning than cluster 2 and 3. Cluster 2 and cluster 3 have similar earning range except that the distribution of schools in cluster 2 has a greater dispersion. 

Next, we plotted median family income against median earning for each cluster.
```{r}
qplot(data = data_km,
      x = MD_FAMINC,
      y = MD_EARN_WNE_P6,
      color = cluster,
      xlab = 'Median Family Income',
      ylab = "Median Earning six years after entry")
```
The plot shows that the median earnings generally increase as median family income increase. But as the family income increase to a certain point, it doesn't have much impact on median earnings after six years.

###do we need this section???  The result is not good. 
To evaluate how good are the clusters we created, we also calculated Silhouette width and Dunn index.
```{r}
#Silhouette width

names(data_km)
data_km['Public']
cluster3.dist.mat = daisy(data_km[,-21]) #get rid of the cluster col.
plot(cluster3.dist.mat )
cluster.sil3 = silhouette(x = as.numeric(data_km$cluster),
                          dist = cluster3.dist.mat)
plot(cluster.sil3)
summary(cluster.sil3)
#Dunn index
dunn(dist = dist(data_final),
     clusters = as.numeric(kmeans(data_km[,-21],3)$cluster))
```
From the plot and the summary of Silhouette width, we see that the silhouette widths for each cluster are  0.31, 0.28 and 0.36 separately. The average Silhouette width for all points is 0.31, minimum width is -0.05, and maximum width is 0.53, which suggest that we are doing OK on clustering, but the results can definitely be improved.

##3.5 Cluster Analysis process - Partitioning around mediods (PAM)

To determine an optimal number of clusters for Partitioning around mediods, we calculated the  silhouette width  for cluster numbers from 2 to 10 and plotted a scree plot. 
```{r}
# Calculate silhouette width for many k using PAM

sil_width <- c(NA)

for(i in 2:10){
  pam_fit <- pam(data_final,diss=FALSE,
                 k = i)
  sil_width[i] <- pam_fit$silinfo$avg.width
}

plot(1:10, sil_width,
     xlab = "Number of clusters",
     ylab = "Silhouette Width")
lines(1:10, sil_width)

```
From the scree plot, we can see the Silhouette Width values are overall low for different number of clusters while the highest value is 0.32 when the number of clusters equals to 2. The low value for Silhouette Width indicates a weak structure for the clustering groups, thus the Partitioning Around Mediods may not be a good approach to conduct clustering analysis in our case. 

Then we locationed two cluster groupings for the data_final data set. 
```{r}
pam_fit <- pam(dist(data_final), k = 2) 
pam_fit3 <- pam(dist(data_final), k = 3) 

plot(pam_fit)
plot(pam_fit3)
pam_fit$clustering

data_final%>%
  mutate(pam_cluster = factor(pam_fit$clustering)) %>%
   {.} -> data_pam

```
The clustering solution is saved in a new column called pam_cluster. 

```{r}
pam_summary = data_pam %>%
  group_by(pam_cluster) %>%
    do(the_summary = summary(.)) 

pam_summary$the_summary
```

```{r}
data_pam %>% 
  ggplot(aes(x=MD_EARN_WNE_P6, y=PCTPELL)) + 
  geom_point(aes(color=pam_cluster)) +
  guides(size = FALSE)
```

Likewise, we look at the distribution of earnings for each cluster.
```{r}
data_pam %>%
  ggplot(aes(x = pam_cluster,
             y = MD_EARN_WNE_P6)) +
  geom_boxplot(aes(fill = pam_cluster)) +
  xlab("Cluster") +
  ylab("Median Earning six years after entry") +
  scale_fill_discrete(name = 'Cluster')
```
It seems that the institutions in cluster 1 have students with higher earnings.

We also plotted median earnings six years after entry again family income for each cluster. 

```{r}
qplot(data = data_pam,
      x = MD_FAMINC,
      y = MD_EARN_WNE_P6,
      color = pam_cluster,
      xlab = 'Median Family Income',
      ylab = "Median Earning six years after entry")
```
The plot shows that cluster 1 has higher post-graduation earning and median family income than cluster2. 


##3.6 Cluster Analysis process- Hierachical 
We used dist function to calcuate the Euclidean distance for our observations and then used hclust function to cluster the observations. 
```{r}
hclust_result = hclust(dist(data_final))
plot(hclust_result, label=data_final$NPT4_Pulic)
```
A dengogram was also requested with the above code block. Due to the large size for our data set, there is not much useful information we could extract from the bottom part of the dengogram. However, there are clealy two very distinct groups at the top of the dengogram. It looks like either two or three groups might be an interesting place to start investigating. 

We first analyzed the cluster memeberships for the three cluster solution.
```{r}
data_final%>%
  mutate(hc_cluster = factor(cutree(hclust_result,k=3))) %>%
   {.} -> data_hc

table(data_hc$hc_cluster)

data_hc %>%
  group_by(hc_cluster) %>%
     dplyr::summarise(COUNT = n()) 

data_hc %>%
  group_by(hc_cluster) %>%
    filter(hc_cluster ==3)
       dplyr::summarise()


```
The solution was recorded in a new column hc_cluster and the updated data set was named with data_hc. 


```{R}
#ggplot
data_hc %>% 
  ggplot(aes(x=MD_EARN_WNE_P6, y=PCTPELL)) + 
  geom_point(aes(color=hc_cluster)) + 
  guides(size=FALSE)
```

Likewise, we explored the distribution of earnings for each cluster.
```{r}
data_hc %>%
  ggplot(aes(x = hc_cluster,
             y = MD_EARN_WNE_P6)) +
  geom_boxplot(aes(fill = hc_cluster)) +
  xlab("Cluster") +
  ylab("Median Earning six years after entry") +
  scale_fill_discrete(name = 'Cluster')
```
It's that cluster 2 has higher earnings than cluster 1 and 3, and about 70% of values in this cluster are above average earning for all school. The earnings for cluster 1 and 3 roughly fall in similar range with cluster 1 spreads more widely, and most of schools in these two clusters are below average earnings.


We also plotted median family income against earning after six years of entry for each cluster.
```{r}
qplot(data = data_hc,
      x = MD_FAMINC,
      y = MD_EARN_WNE_P6,
      color = hc_cluster,
      xlab = 'Median Family Income',
      ylab = "Median Earning six years after entry")
```
We see that most observations fall into cluster 1 and cluster 3. Like the the clusters before, cluster 1 tend to have lower family income and median earnings than cluster 3.


## 3.8 Cluster Validation and Comparison 
To validate different clustering methods and choice of number of clusters, we ran a cluster validation using *clValid* function. It allows us to compare different clustering evaluation metrics under different clustering approach and number of clusters.
```{r}
#rownames(data_final) = 1:4528
methods.vec = c("hierarchical","kmeans","pam")
clValid.result = clValid(data_final,
                         2:5,
                         clMethods=methods.vec,
                         validation="internal",
                         maxitems = 2000000)

summary(clValid.result)
```
The result shows that the best choice of number of cluster is 2, and the best method is hierarchical.As stated in the 3.XXX, either 2 cluster solutions or 3 cluster solutions may be applicable for the hierarchical clustering. In the following session, we will campare these two solutions in detailes.

We used the following codes to analyze he cluster memeberships for the two cluster solution.
```{r}

data_final%>%
  mutate(hc_cluster_2 = factor(cutree(hclust_result,k=2)),
         hc_cluster_3 = factor(cutree(hclust_result,k=3))) %>%
   {.} -> data_hc



table(data_hc$hc_cluster)

data_hc %>%
  group_by(hc_cluster) %>%
     dplyr::summarise(COUNT = n()) 



data_hc %>%
  group_by(hc_cluster) %>%
    filter(hc_cluster ==2) %>%
     {.} -> hc_cluster_2

data_hc %>%
  group_by(hc_cluster) %>%
    filter(hc_cluster ==1) %>%
     {.} -> hc_cluster_1



hc_cluster_1


subset(data_hc,hc_cluster==2)
subset(data_hc,hc_cluster==1)


data.with.rownames[c("475750", "476230"),]

subset(data_c, UNITID %in% rownames(subset(data_hc,hc_cluster==1)))

    
```



## 3.9 Findings and Conlusions 
To conclude, after trying K means, PAM and hierarchy methods to perform clustering analysis, we decided that the optimal method is hierarchy with 2 clusters. 

#4 Further Studies


