---
title: "MA710-A2- College Scorecard Association Rules & Cluster Analysis"
author: "Xiang Li, Xue Zhou"
date: "March 11, 2017"
output: html_document
---

#Table of Contents
* 1.[Introduction](#Introduction)
* 2.[Association Rule Analysis]
  * 2.1 [Goal]
  * 2.2 [Data Preparation]
  * 2.3 [Objective]
  * 2.4 [Association Rule Analysis - Median earning six years after entry]
  * 2.5 [Association Rule Analysis - Predominant Degree]
  * 2.6 [Findings and Conclusions]
* 3.[Cluster Analysis]
  * 3.1 [Goal]
  * 3.2 [Data Preparation]
  * 3.3 [Objective]
  * 3.4 [Cluster Analysis -K means]
  * 3.5 [Cluster Analysis -PAM]
  * 3.6 [Cluster Analysis -DBSCAN]
  * 3.7 [Cluster Analysis -Hierachical]
  * 3.8 [Cluster Validation and Comparison]
  * 3.9 [Findings and Conclusions]
* 4[Further Studies]
       


```{r setup, include=FALSE,warning=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message = FALSE)
```

#1. Introduction
    In this stage, we will use some unsupervised learning techniques to analyze the College Scorecard Data. 
    The main techniques we use are association rule mining and cluster analysis.
    
#2. Association Rule Analysis
## 2.1 Goal 
    In the assignment, we've explored the relationship between pairs of variables selected from the full CollegeScorecard dataset. Now we proceed to study two variables that most interested us: earning and predominant degree type. We will use association rule mining technique to explore relationship between post-graduation earning, predominant degree and other variables. We used the cleaned dataset(data_clean.csv) in the Assignment 1 as our original dataset in this stage. 
    

## 2.2 Data Preparation 
 First, we loaded all the packages required for analysis as belows: 
```{r}
library(readr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(ggvis)
library(plyr)
library(arules)
library(arulesViz)
library(cluster)
library(clValid)
```

Then imported the dataset
```{r}
data = read.csv("data_clean.csv",
                 header = TRUE, na.strings = 'NA')
names(data)
```

Considering the column X, UNITID, INSTNM_factor and MD_EARN_WNE_P10 are not related to our analysis in this stage, we removed these four variables from the dataset. Then we renamed the column names to make them more readable. The output returned is clean data set we can use in the later analysis.


```{r}
data_asso = data[,-c(1,2,3,17)]
#data_asso = na.omit(data_asso)
data_asso %>% dplyr::rename(state = STABBR_factor,
                     pred_degree = PREDDEG_factor,
                     control = Control_factor,
                     net_cost = NPT4_COMBINE,
                     per_independent = DEP_STAT_PCT_IND,
                     per_1generation = PAR_ED_PCT_1STGEN,
                     median_family_inc =MD_FAMINC,
                     per_pell = PCTPELL,
                     per_loan = PCTFLOAN,
                     debt_grad = GRAD_DEBT_MDN,
                     debt_non_grad = WDRAW_DEBT_MDN,
                     per_app_greater2 = APPL_SCH_PCT_GE2,
                     median_earning_6years = MD_EARN_WNE_P6,
                     repayment_rate = RPY_3YR_RT_SUPP,
                     default_rate = CDR3) %>%
                     {.} -> data_asso

```

To make the dataset applicable for association rules mining, we need to encode all the numeric variables into categorical variables with appropriate levels. We use the following function *make.ntiles* to perform this conversion.  

```{r}
# create factor variables from all of the numeric variables you chose to work with using the make.ntiles function.

make.ntiles = function (inputvar, n) {
  inputvar %>%
quantile(.,
(1/n) * 1:(n-1),
             na.rm=TRUE
    ) %>%
c(-Inf, ., Inf) %>% cut(inputvar,
breaks=.,
paste("Q", 1:n, sep="") )
}
```

With the above functions, we are able to convert all the numeric variables into a factor variable with n levels. In our case, we decided to encode all the numeric variables into 3 categories, thus n =3. 


Then we applied the make.ntiles function on every numeric column.The resulting output would be a set of variables encoded from numeric variables according to their quantiles.Then we combined the encoded variables with the original three categorical variables in the dataset to generate a complete dataset. 

```{r}
data_asso %>% 
  sapply(.,is.numeric) %>%
    data_asso[,.] %>%
      apply(.,make.ntiles, n=3, MARGIN = 2) %>%
         as.data.frame()   %>%
             {.} -> data_num
data_clean = cbind(data_num,data_asso[,c(1,2,3)])
#removed all the missing data. 
#data_clean = na.omit(data_combine)
```
As a conclusion, the dataset *data_clean* would be the prepared dataset we would use in the further association rule mining. 


## 2.3 Objective 
   Specifically, our two objectives for this part of analysis are: 
      1. To identify factors that have notable relationship with post-graduate earning so that they can be used later to predict the post-graduate earnings. 
      2. To describe the profile of university awarding different types degree using related attributes. 
    
## 2.4 Association Rule Analysis - Median earning six years after entry

We used the following code to generate the association rules for the *data_clean* dataset. We used the parameter, appearance and control parameters of the apriori command below to modify the rules it generates. In this case, the appearance parameter indicates that the consequent (right hand side) of all generated rules should be one of the three values of the median_earning_6years variable. The parameter parameter indicates that all rules generated by this command must have support greater than 0.05 and confidence greater than 0.8. The minlen and maxlen parameters specify that all generated rules with a length between 2 to 4. ###Since the consequent (right hand side) always has one value then the antecedent (left hand side) will always have one to three values.


```{r}
#filter the rhs to the MD_EARN_WNE_P6.F column.

apriori.appearance_e = list(rhs=c('median_earning_6years=Q1','median_earning_6years=Q2','median_earning_6years=Q3'), default='lhs')
apriori.parameter_e = list(support=0.05,
                         confidence=0.8,minlen=2, maxlen=4)
apriori.control_e = list(verbose=FALSE)
rules_earning = apriori(data_clean,   
                 parameter=apriori.parameter_e,
                 appearance=apriori.appearance_e,
                 control=apriori.control_e)
length(rules_earning)
inspect(rules_earning)


```
In total, there are 95 rules generated based on the parameters we specified before. 

A rule is redundant if a more general rules with the same or a higher predictive power exists. A more specific rule is redundant if it is only equally or even less predictive than a more general rule. We will use *lift* messure to determine the redundant rules. Considering the redundant rules provide no extra information in addition to the valid rules, we only kept the rules which were not redundant. 

```{r}

rules.sorted <- sort(rules_earning, by="lift") 

length(rules.sorted)
# quality(rules.sorted )$improvement <- interestMeasure(rules.sorted , measure = "improvement")
# is.redundant(rules.sorted)

## redundant rules
rules_redundant = rules.sorted[is.redundant(rules.sorted)]

## non-redundant rules
rules_pruned = rules.sorted[!is.redundant(rules.sorted)] 

rules_redundant 
rules_pruned 

```
From the output, there are 8 redundant rules in this case. Thus we would only focus on the subset of rules with the remaining 87 rules. 


Next we sort the resulting rules by *support* to take a further analysis and we specifically focused on the top 5 rules. 

```{r}
#sort by support, lift and confidence
rules_support = sort(rules_pruned,by="support", decreasing = T)
inspect(rules_support[1:5])

plot(rules_support[1:5], method = "graph")
plot(rules_support[1:5], method = "graph", control = list(type = "items"))
plot(rules_pruned, method = "grouped")

```
Based on the output, the rules with the highest support value all have consequent value as median_earning_6years=Q3. If we take the first rule as an example. The support value of 0.1256 means that the frequency of the itemset in this rule is 12.56% - in the dataset, 12.56% of the universities are ones with high median family income, high debt amount before graduation, high repayment rate and high median earning 6 years after they were enrolled. The confidence value of 0.8091 means that 80.91% of the universities whose students have high median family income, high debt amount before graduation and high repayment rate are the universities whose students have high median earning 6 years after the enrollment.The lift value of 3.23 means that univeristies qualified for both antecedent and consequent conditions are 223% higher than we would expect if antecedent and consequent are independent. The high value of confidence and lift indicates that the rule has a good performance.  


Then we ordered the pruned rules by the measurement of confidence an pulled out the top 5 rules. 
```{r}
rules_confidence = sort(rules_pruned,by="confidence", decreasing = T)
inspect(rules_confidence[1:5])


```
A high value of confidence means that the number of times the if/then statement have been found to be true is high. If we take the first rule as example, the support value of 0.9094 means that 90.94% of the univrsities who have a low percentage in Pell grant, high value in debt_non_grad and low value in default rate are the universities with a high median earning value. The high value of lift(3.63) also indicates that this rule is useful in finding consequent items sets.  


Similarly, we sorted the rules by lift value and displayed the top 5 rules. 
```{r}
rules_lift = sort(rules_earning,by="lift", decreasing = T)
inspect(rules_lift[1:5])
```
From the output, all the 5 resulting rules have the lift value large than 1, indicating that all five rules are useful in finding the consequent item sets. The acceptable value for support and confidence are all indications of notable relationship between values of variables. 




Next, we requested scatterplot, balloon plot, graph and parallel coordinates plot to visualize the resulting 87 association rules. We used the scatterplot, balloon plot, graph and parallel coordinates plot to visualize the association rules as below.
```{r}

plot(rules_pruned)
plot(rules_pruned, method="grouped")
plot(rules_pruned, method="graph")
plot(rules_pruned, method="graph", control=list(type="items"))
plot(rules_pruned, method="paracoord", control=list(reorder=TRUE))
```


============================================================================================================
## 2.5 Association Rule Analysis - Predominant Degree

At the beginning of our association rule analysis, we set some parameters that can generate desirable output for use.The right-hand side is set to predominant degree, which indicates we only study what factors may be more likely to exist with different degree types simultaneously. 
```{r}
apriori.appearance = list(rhs = c("pred_degree=Bachelor's-degree","pred_degree=Certificate-degree",
                                  "pred_degree=Associate's-degree","pred_degree=NotClassified",
                                  "pred_degree=Graduate-degree"),
                          default = 'lhs')
```

Then we set the minimum support to 0.1, which means that only combination existing more than a frequency of 10% will be chosen.Likewise, confidence is set to 0.8, indicating that only rules with conditional probability greater than 0.5 will be chosen.

```{r}
apriori.parameter = list(support = 0.10,
                          confidence =0.8)
apriori.control = list(verbose = FALSE)
rules_degree = apriori(data_clean,
                parameter = apriori.parameter,
                appearance = apriori.appearance,
                control = apriori.control)
length(rules_degree)
```
In total, we get 225 association rules. Like what we did before, we also removed redundant rules.

```{r}
rules_degree_redundant = rules_degree[is.redundant(rules_degree)]

## non-redundant rules
rules_degree_pruned = rules_degree[!is.redundant(rules_degree)] 

rules_degree_redundant 
rules_degree_pruned

```
After removing redundant rules, we get in total 195. We are going to study these rules in the following part.


We first sort all 195 rules by their lift. Lift indicates the ratio of the observed support to that expected if two items were independent. If the lift equals to 1, it would imply that the probability of occurrence of antecedent and that of the consequent are indenpend of each other. With that said, the higher lift is, the more likely two things are dependent.
```{r}
inspect(sort(rules_degree_pruned,by='lift')[1:5])
```
Let's take first rule as an example. A school with low percentage of first generation student, student having high family income, low percentage of pell grant, high percentage of student applying to more than  one school, high percentage of repayment rate leads to a bachelor-degree predominant. A 0.102 means that about 10% of schools satisfy the description above. A confidence 0.97 indicates that among all schools with attributes at left-handed side, 97% would offer bachelor degree as their predominant degree. A lift as high as 3.5 shows that the probability of occurance of this rule is 3.5 times the expected probability if left-handed side and right-handed side are independent.   

Then we look at the rules sorted by confidence.
```{r}
inspect(sort(rules_degree_pruned,by='confidence')[1:5])
```

We realized that the top rule in terms of strength of confidence is also the top rule when we look at the lift, which indicates that this rule has a good performance.

Lastly, we look at rules sorted by support. This time we got a different rule at the top place. A support 0.19 means that about 20% of schools have attending students with low debt, belong to private for-profit school and have certificate degree as their predominant degree. This rule has relatively low confidence and lift than rules we discussed earlier but on the other hand it generalize better than other rules since it has a higher support. 

```{r}
inspect(sort(rules_degree_pruned,by='support')[1:5])
```
 
We ploted the support versus confidence and lift of all the rules to see if there is any relationship between them.
```{r}
plot(rules_degree_pruned, method = NULL, measure = "support", shading = "lift",
      interactive = FALSE, data = NULL, control = NULL)
```

The plot seems to show a trend that as support increases, both confidence and lift decreases.

Then we use some visualizations to help us better understand these rules. I used itemset plot here so that we can clearly see which attributes lead to which consequence. To make plot clear to see, we only plot rules with a support greater than 0.15. An alternative way to visualize these rules is to build a interactive shinny app developed by Andrew Brooks. (http://brooksandrew.github.io/simpleblog/articles/association-rules-explore-app/)

```{r}
subrules <- rules_degree_pruned[quality(rules_degree_pruned)$support >0.15]
subrules2 <- head(sort(rules_degree_pruned, by="lift"), 5)
plot(subrules2,method="graph", control=list(type="itemsets"))
```
The plot shows the rules with four rules with top support. The right-handed side of these rules are both Bachelor degree. It seems that we get some good attributes to profile a Bachelor-predomint school. But we also want to see what attributes are related to other types of schools. To do this, we segement association rules by their right-handed side. 195 rules all belong to either bachelor group or certificate, which means for other degree type, we couldn't find any statistically significant rules to decribe their profile.


```{r}
rules_bachelor <- subset(rules_degree_pruned, (rhs %in% c("pred_degree=Bachelor's-degree")))
rules_bachelor
inspect(sort(rules_bachelor,by='lift')[1:5])
plot(sort(rules_bachelor,by='lift')[1:5],method="graph", control=list(type="itemsets"))
```

There are 185 rules with pred_degree=Bachelor's-degree as their right-hand side, which indicates that more significant relationship exist between bachelor degree are other variables.Taking the first rule as an example, schools with students whose family has a high income,low percentage of students applying for pell-grant, and low default rate are more likely to be classified as bachelor degree predomint school. This imply a positive relationship between the level of education and students' wealth situation-students in school offering high degree are more likely to come from wealthier family.

Show rules for certificate degree.
```{r}

rules_certificate <- subset(rules_degree_pruned, (rhs %in% c("pred_degree=Certificate-degree")))
rules_certificate
inspect(sort(rules_certificate,by='lift')[1:5])
plot(sort(rules_certificate,by='lift')[1:5],method="graph", control=list(type="itemsets"))
```
There are 10 rules for certificate degree. Also taking the first rule as an example, institutions with attending students whose debt are low, and have a control type equal to private for-profit are morely to classified as certificate-degree predominant. Looking at the first five rules, we also notice that these institutions usually have graduation with low post-graduation earning, and low debt after completion.



## 2.6 Conclusions

To conclude, all the relationship we found belong to bachelor-predominant school or certificate-predominant, which means we identify some attributes that can be used to profile these two tyoes of schools. For with institution with bachelor as their most awarding degree, students are more likely to come from wealthier family with educated parents, less likely to apply for pell grant, but willing to take more debts for their education.

For institutions with certificate as their most awarding degree, these schools are more likely to be private for-profit school. Students in these school usually usually have a lower debt amount and lower earnings six years after graduation compared to other type of school. 



#3. Cluster Analysis 
## 3.1 Goal
   The goal of cluster analysis is to find groups (clusters) of rows where
     1. The distance between any two rows in a single cluster is small
     2. The distance between any two rows in different clusters is large
    In our case, the goal of this chapter is to find natural groups of similar institutions and to describe or characterize the institutions in these groups. 
   

## 3.2 Data Preparation

Similar as the last section, we used the data_clean in assignment 1 as our original dataset. Then we deleted the first four columns and renamed the row names with the university ID. 

```{R}
data_c = read.csv("data_clean.csv",
                 header = TRUE, na.strings = 'NA')

#get rid of ID, university name and state columns, rename the rownames as the university ID. 
data.with.rownames <- data.frame(data_c[,-c(1:4)], row.names=data_c[,2])
names(data.with.rownames)
glimpse(data.with.rownames)
```
The data_c dataset now contained 15 variables with 7793 observations. 


In order for a yet-to-be-chosen algorithm to group observations together, we first need to define some notion of (dis)similarity between observations. Considering that most popular choice for clustering like Euclidean distance is only valid for continuous variables, we need to convert the factor variables into binary dummy variables. 

```{R}
# Create the dummy boolean variables using the model.matrix() function.
dummy_preddeg = model.matrix(~PREDDEG_factor-1, data.with.rownames)
dummy_control = model.matrix(~Control_factor-1, data.with.rownames)
dummy_preddeg 
dummy_control

#rename the coloumn names for dummay variables to make them more readable. 
colnames(dummy_preddeg ) <- gsub("PREDDEG_factor","",colnames(dummy_preddeg))
colnames(dummy_control  ) <- gsub("Control_factor","",colnames(dummy_control))
dummy_preddeg 
dummy_control

#Combine the matrix back with the original dataframe.
data_combine_c= cbind(data.with.rownames, dummy_preddeg,dummy_control) 

#git rid of the factor coloumns which have been converted to the dummy variable.
data_ready = data_combine_c[,-c(1:2)]
data_ready
glimpse(data_ready)

#remove the missing values
data_ready %>%
  na.omit(data_ready) %>%
  {.} -> data_clean_c
data_clean_c


```

All the columns in the dataset data_clustering are continuous variables, which are applicable for calculating the (dis)similarity between observations. To ensure the clustering analysis gives each variable the same weight, we decide to standardize every numeric variable in the dataset. Also, the column 17 only contains 0, which can not provide any information for us, so we decided to remove this column.

```{R}

#delete the 17 col since its all 0 
data_final= data.frame(scale(data_clean_c[,-17]))
names(data_final)

```
The data_final will be used as our final dataset to conduct further clustering analysis. 

## 3.3 Objective

############Clustering Analysis Start#####################

## 3.4 Cluster Analysis process-  K-menas cluster analysis 
There are three main widely used Clustering techniques: K-means, Partitioning Around Medoids (PAM) and DBSCAN. We implemented the three techniques one by one as follows. 

To reproduce the results, we set the seed value to 100 as the code below.
``` {r}
#set random seed to reproduce the results
set.seed(100)
```



To determine the opimum k value for clustering the observations, we drawed a scree plot as follows:
```{R}

# Initialise ratio_ss
ratio_ss <- rep(0, 7)

# Finish the for-loop
for (k in 1:7) {
  # Apply k-means to data_final: data_km
   data_km <- kmeans(data_final, k, nstart = 20)
  # Save the ratio between of WSS to TSS in kth element of ratio_ss
  ratio_ss[k] <- data_km$tot.withinss / data_km$totss
}

# Make a scree plot with type "b" and xlab "k"
plot(ratio_ss, type = "b", xlab = "k")


## choose K, find k that minimizes WSS: scree plot
##tot.withiss   : wss
## betweenss: bss


```

from the scree plot, we can tell that the plot shows a considerable drop for k equal to 3. We will choose k=3 as out optimal k value. 

Then we use kmeans function with the cluter number of 3. 
```{R}
# Cluster the data using k-means: data_km2. 3 clusters, repeat 20 times

km_result <- kmeans(data_final, 3, nstart = 20)
names(data_final)
km_result$cluster

#interpretation with descriptive statistics: the summary data for the 3 resulting clusters
data_final%>%
  mutate(cluster = factor(km_result$cluster)) %>%
   {.} -> data_km

km_summary = data_km %>%
  group_by(cluster) %>%
    do(the_summary = summary(.)) 

km_summary$the_summary
```
The output is the summary of each column for each cluster.

Then we created some visualizations to help us better understand each cluster. We first plot median earning after six years against percentage of pell grant for each cluster.
```{r}
# Plot the observations with Color using clusters
data_km %>% 
  ggplot(aes(x=MD_EARN_WNE_P6, y=PCTPELL)) + 
  geom_point(aes(color=cluster)) + 
  guides(size=FALSE)
```

Then we look at the earning distribution for each cluster by creating a box plot. 

```{r}
names(data_km)

data_km %>%
  ggplot(aes(x = cluster,
             y = MD_EARN_WNE_P6)) +
  geom_boxplot(aes(fill = cluster)) +
  xlab("Cluster") +
  ylab("Median Earning six years after entry") +
  scale_fill_discrete(name = 'Cluster')
```
It seems that institutions in cluster 1 have student with higher post-graduation earning than cluster 2 and 3. Cluster 2 and cluster 3 have similar earning range excep that the distribution of schools in cluster 2 has a greater dispersion. 

Next, we plot median family income against median earning for each cluster.
```{r}
qplot(data = data_km,
      x = MD_FAMINC,
      y = MD_EARN_WNE_P6,
      color = cluster,
      xlab = 'Median Family Income',
      ylab = "Median Earning six years after entry")
```
The plot shows that the median earnings generally increase as median family income increase. But as the family income increase to a certain point, it doesn't have much impact on median earnings after six years.

To evaluate how good are the clusters we created, we also calculated Silhouette width and Dunn index.
```{r}
#Silhouette width
cluster3.dist.mat = daisy(data_km[,-21])
cluster.sil3 = silhouette(x = as.numeric(data_km$cluster),
                          dist = cluster3.dist.mat)
plot(cluster.sil3)
summary(cluster.sil3)
#Dunn index
dunn(dist = dist(data_final),
     clusters = as.numeric(kmeans(data_km[,-21],3)$cluster))
```
From the plot and the summary of Silhouette width, we see that the silhouette widths for each cluster are  0.31, 0.28 and 0.36 separatley. The average Silhouette width for all points is 0.31, mininum width is -0.05, and maximum width is 0.53, which suggest that we are doing ok on clustering, but the results can definitely be improved.

##3.5 Cluster Analysis process - Partitioning around mediods (PAM)
```{r}

# Calculate silhouette width for many k using PAM
# see mixed links online

sil_width <- c(NA)

for(i in 2:10){
  pam_fit <- pam(data_final,
                 diss = TRUE,
                 k = i)
  sil_width[i] <- pam_fit$silinfo$avg.width
}
```
# Plot sihouette width (higher is better)

```{r}
plot(1:10, sil_width,
     xlab = "Number of clusters",
     ylab = "Silhouette Width")
lines(1:10, sil_width)

pam_fit <- pam(data_final, diss = TRUE, k = 2) 
names(pam_fit)
pam_fit$clustering

data_final%>%
  mutate(pam_cluster = factor(pam_fit$clustering)) %>%
   {.} -> data_pam

pam_summary = data_pam %>%
  group_by(pam_cluster) %>%
    do(the_summary = summary(.)) 

pam_summary$the_summary
```

```{r}
data_pam %>% 
  ggplot(aes(x=MD_EARN_WNE_P6, y=PCTPELL)) + 
  geom_point(aes(color=pam_cluster)) +
  guides(size = FALSE)
```

Likewise, we look at the distribution of earnings for each cluster.
```{r}
data_pam %>%
  ggplot(aes(x = pam_cluster,
             y = MD_EARN_WNE_P6)) +
  geom_boxplot(aes(fill = pam_cluster)) +
  xlab("Cluster") +
  ylab("Median Earning six years after entry") +
  scale_fill_discrete(name = 'Cluster')
```
It seems that the institutions in cluster 1 have students with higher earnings.

We also ploted median earnings six years after entry again family income for each cluster. 

```{r}
qplot(data = data_pam,
      x = MD_FAMINC,
      y = MD_EARN_WNE_P6,
      color = pam_cluster,
      xlab = 'Median Family Income',
      ylab = "Median Earning six years after entry")
```
The plot shows that cluster 1 has higher post-graduation earning and median family income than cluster2. 


##3.6 Cluster Analysis process - DBSCAN

```{R}

library(fpc)

dbscan.result = dbscan(data=data_final,eps=0.42,MinPts=10)
dbscan.result$cluster
```


##3.7 Cluster Analysis process- Hierachical 
```{r}

hclust_result = hclust(dist(data_final))
plot(hclust_result, label=data_final$NPT4_Pulic)

data_final%>%
  mutate(hc_cluster = factor(cutree(hclust_result,k=3))) %>%
   {.} -> data_hc

names(data_hc)

#ggplot
data_hc %>% 
  ggplot(aes(x=MD_EARN_WNE_P6, y=PCTPELL)) + 
  geom_point(aes(color=hc_cluster, size=2)) + 
  guides(size=FALSE)
```
Likewise, we explored the distribution of earnings for each cluster.
```{r}
data_hc %>%
  ggplot(aes(x = hc_cluster,
             y = MD_EARN_WNE_P6)) +
  geom_boxplot(aes(fill = hc_cluster)) +
  xlab("Cluster") +
  ylab("Median Earning six years after entry") +
  scale_fill_discrete(name = 'Cluster')
```
It's that cluster 2 has higher earnings than cluster 1 and 3, and about 70% of values in this cluster are above average earning for all school. The earnings for cluster 1 and 3 roughly fall in similar range with cluster 1 spreads more widely, and most of schools in these two clusters are below average earnings.


We also ploted median family income against earning after six years of entry for each cluster.
```{r}
qplot(data = data_hc,
      x = MD_FAMINC,
      y = MD_EARN_WNE_P6,
      color = hc_cluster,
      xlab = 'Median Family Income',
      ylab = "Median Earning six years after entry")
```
We see that most observations fall into cluster 1 and cluster 3. Like the the clusters before, cluster 1 tend to have lower family income and median earnings than cluster 3.


## 3.8 Cluster Validation and Comparison 

To compare different clustering methods and choice of number of clusters, we ran a cluster validation using *clValid* function. It allows us to compare differnt clustering evaluation metrics under different clustering approach and number of clusters.
```{r}
rownames(data_final) = 1:4528
methods.vec = c("hierarchical","kmeans","pam","agnes")

clValid.result = clValid(data_final,
                         3:6,
                         clMethods=methods.vec,
                         validation="internal",
                         maxitems = 2000000)

summary(clValid.result)
```
The result shows that the best choice of number of cluster is 3. And the best method is hierarchy.

## 3.9 Findings and Conlusions 

To conclude, after trying Kmeans, PAM, DBSCAN and hierarchy methods to perform clustering analysis, we decided that the optimal method is hierarchy with 3 clusters. ```Talk about summary statistics here``

#4 Further Studies


