---
title: "MA710-A2- College Scorecard Association Rules & Cluster Analysis"
author: "Xiang Li, Xue Zhou"
date: "March 11, 2017"
output: html_document
---

#Table of Contents
* 1.[Introduction](#Introduction)
* 2.[Association Rule Analysis]
  * 2.1 [Goal]
  * 2.2 [Data Preparation]
  * 2.3 [Objective]
  * 2.4 [Association Rule Analysis - Median earning six years after entry]
  * 2.5 [Association Rule Analysis - Predominant Degree]
  * 2.6 [Findings and Conclusions]
* 3.[Cluster Analysis]
  * 3.1 [Goal]
  * 3.2 [Data Preparation]
  * 3.3 [Objective]
  * 3.4 [Cluster Analysis -K means]
  * 3.5 [Cluster Analysis -PAM]
  * 3.6 [Cluster Analysis -DBSCAN]
  * 3.7 [Cluster Analysis -Hierachical]
  * 3.8 [Cluster Validation and Comparison]
  * 3.9 [Findings and Conclusions]
* 4[Further Studies]
       


```{r setup, include=FALSE,warning=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message = FALSE)
```

#1. Introduction
    In this stage, we will use some unsupervised learning techniques to analyze the College Scorecard Data. 
    The main techniques we use are association rule mining and cluster analysis.
    
#2. Association Rule Analysis
## 2.1 Goal 
    we will use association rule mining technique to explore relationship between post-graduation earning, predominant degree and other variables. We used the cleaned dataset(data_clean.csv) in the Assignment 1 as our original dataset in this stage. 
    

## 2.2 Data Preparation 
 First, we loaded all the packages required for analysis as belows: 
```{r}
library(readr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(ggvis)
library(plyr)
library(arules)
library(arulesViz)
library(cluster)
library(clValid)
```

Then imported the dataset
```{r}
data = read.csv("data_clean.csv",
                 header = TRUE, na.strings = 'NA')
names(data)
```

Considering the column X, UNITID, INSTNM_factor and MD_EARN_WNE_P10 are not related to our analysis in this stage, we removed these four variables from the dataset. Then we renamed the column names to make them more readable. The output returned is clean data set we can use in the later analysis.


```{r}
data_asso = data[,-c(1,2,3,17)]
#data_asso = na.omit(data_asso)
data_asso %>% dplyr::rename(state = STABBR_factor,
                     pred_degree = PREDDEG_factor,
                     control = Control_factor,
                     net_cost = NPT4_COMBINE,
                     per_independent = DEP_STAT_PCT_IND,
                     per_1generation = PAR_ED_PCT_1STGEN,
                     median_family_inc =MD_FAMINC,
                     per_pell = PCTPELL,
                     per_loan = PCTFLOAN,
                     debt_grad = GRAD_DEBT_MDN,
                     debt_non_grad = WDRAW_DEBT_MDN,
                     per_app_greater2 = APPL_SCH_PCT_GE2,
                     median_earning_6years = MD_EARN_WNE_P6,
                     repayment_rate = RPY_3YR_RT_SUPP,
                     default_rate = CDR3) %>%
                     {.} -> data_asso

```

To make the dataset applicable for association rules mining, we need to encoded all the numeric variables into categorical variables with appropriate levels. We use the following function *make.ntiles* to implement the conversion.  

```{r}
# create factor variables from all of the numeric variables you chose to work with using the make.ntiles function.

make.ntiles = function (inputvar, n) {
  inputvar %>%
quantile(.,
(1/n) * 1:(n-1),
             na.rm=TRUE
    ) %>%
c(-Inf, ., Inf) %>% cut(inputvar,
breaks=.,
paste("Q", 1:n, sep="") )
}
```

With the above functions, we are able to convert all the numeric variables into a factor variable with n levels. In our case, we decided to encode all the numeric variables into 3 categories, thus n =3. 


Then we applied the make.ntiles function on every numeric column.The resulting output would be a set of variables encoded from numeric variables according to their quantiles.Then we combined the encoded variables with the original three categorical variables in the dataset to generate a complete dataset. 

```{r}
data_asso %>% 
  sapply(.,is.numeric) %>%
    data_asso[,.] %>%
      apply(.,make.ntiles, n=3, MARGIN = 2) %>%
         as.data.frame()   %>%
             {.} -> data_num
data_clean = cbind(data_num,data_asso[,c(1,2,3)])
#removed all the missing data. 
#data_clean = na.omit(data_combine)
```
As a conclusion, the dataset *data_clean* would be the prepared dataset we would use in the further association rule mining. 


## 2.3 Objective 
   Specifically, our two objectives for this part of analysis are: 
      1. To identify factors that have notable relationship with post-graduate earning so that they can be used later to predict the post-graduate earnings. 
      2. To Describe the profile of university awarding different types degree with related attributes. 
    
## 2.4 Association Rule Analysis - Median earning six years after entry

We used the following code to generate the association rules for the *data_clean* dataset. We used the parameter, appearance and control parameters of the apriori command below to modify the rules it generates. In this case, the appearance parameter indicates that the consequent (right hand side) of all generated rules should be one of the three values of the median_earning_6years variable. The parameter parameter indicates that all rules generated by this command must have support greater than 0.05 and confidence greater than 0.8. The minlen and maxlen parameters specify that all generated rules contain 2 ~4 values. ###Since the consequent (right hand side) always has one value then the antecedent (left hand side) will always have one to three values.


```{r}
#filter the rhs to the MD_EARN_WNE_P6.F column.

apriori.appearance_e = list(rhs=c('median_earning_6years=Q1','median_earning_6years=Q2','median_earning_6years=Q3'), default='lhs')
apriori.parameter_e = list(support=0.05,
                         confidence=0.8,minlen=2, maxlen=4)
apriori.control_e = list(verbose=FALSE)
rules_earning = apriori(data_clean,   
                 parameter=apriori.parameter_e,
                 appearance=apriori.appearance_e,
                 control=apriori.control_e)
length(rules_earning)
inspect(rules_earning)


```
In total, there are 95 rules generated based on the parameters we specified before. 

A rule is redundant if a more general rules with the same or a higher predictive power exists. A more specific rule is redundant if it is only equally or even less predictive than a more general rule. We will use *lift* messure to determine the redundant rules. Considering the redundant rules provide no extra information in addition to the valid rules, we only kept the rules which were not redundant. 

```{r}

rules.sorted <- sort(rules_earning, by="lift") 
quality(rules.sorted )$improvement <- interestMeasure(rules.sorted , measure = "improvement")
is.redundant(rules.sorted)

## redundant rules
rules_redundant = rules.sorted[is.redundant(rules.sorted)]

## non-redundant rules
rules_pruned = rules.sorted[!is.redundant(rules.sorted)] 

rules_redundant 
rules_pruned 

```
From the output, there are 8 redundant rules in this case. Thus we would only focus on the subset of rules with the remaining 87 rules. 




Next we sort the resulting rules by *support* to take a further analysis and we specifically focused on the top 5 rules. 

```{r}
#sort by support, lift and confidence
rules_support = sort(rules_pruned,by="support", decreasing = T)
inspect(rules_support[1:5])

plot(rules_support[1:5], method = "graph")
plot(rules_support[1:5], method = "graph", control = list(type = "items"))
plot(rules_pruned, method = "grouped")

```
Based on the output, the rules with the highest support value are all 

```{r}
rules_confidence = sort(rules_pruned,by="confidence", decreasing = T)
inspect(rules_confidence[1:5])


```

```{r}
rules_lift = sort(rules_earning,by="lift", decreasing = T)
inspect(rules_lift[1:5])
```

Visualization
```{r}
#scatter plot for all the rules
plot(rules_earning)
```

```{r}
#grouped matrix for all the rules
plot(rules_earning, method = "grouped")
plot(rules_earning, method = "graph")
plot(rules_earning, method = "graph", control = list(type = "items"))
 plot(rules_earning, method = "paracoord", control = list(reorder = TRUE))
```


## 2.5 Association Rule Analysis - Predominant Degree

At the beginning of our association rule analysis, we set some parameters that can generate desirable output for use.The right-hand side is set to predominant degree, which indicates we only study what factors may be more likely to exist with different degree types simultaneously. 
```{r}
apriori.appearance = list(rhs = c("pred_degree=Bachelor's-degree","pred_degree=Certificate-degree",
                                  "pred_degree=Associate's-degree","pred_degree=NotClassified",
                                  "pred_degree=Graduate-degree"),
                          default = 'lhs')
```

Then we set the minimum support to 0.1, which means that only combination existing more than a frequency of 10% will be chosen.Likewise, confidence is set to 0.5, indicating that only rules with conditional probability greater than 0.5 will be chosen.

```{r}
apriori.parameter = list(support = 0.10,
                          confidence =0.8)
apriori.control = list(verbose = FALSE)
rules_degree = apriori(data_clean,
                parameter = apriori.parameter,
                appearance = apriori.appearance,
                control = apriori.control)
length(rules_degree )
```
In total, we get 225 association rules.

```{r}
inspect(sort(rules_degree,by='lift')[1:5])
inspect(sort(rules_degree,by='confidence')[1:5])
inspect(sort(rules_degree,by='support')[1:5])
```

If we take a look at first five association rules ordered by lift, we notice that the right-hand side of these five rules are all bachelor degree, and they all have relatively low support. Plot the support versus lift of all the rules.
```{r}
plot(rules_degree, method = NULL, measure = "support", shading = "lift",
      interactive = FALSE, data = NULL, control = NULL)
```

Visuallly represented

```{r}
subrules <- rules_degree[quality(rules_degree)$confidence > 0.8 & quality(rules_degree)$support >0.15]
subrules2 <- head(sort(rules_degree, by="lift"), 5)
plot(subrules2,method="graph", control=list(type="itemsets"))
```

show rules for bachelor degree
```{r}
rules_bachelor <- subset(rules_degree, (rhs %in% c("pred_degree=Bachelor's-degree")))
inspect(sort(rules_bachelor,by='lift')[1:5])
plot(sort(rules_bachelor,by='lift')[1:5],method="graph", control=list(type="itemsets"))
```

There are 80 rules with pred_degree=Bachelor's-degree as their right-hand side, which indicates that more significant relationship exist between bachelor degree are other variables.Taking the first rule as an example, schools with students whose family has a high income,low percentage of students applying for pell-grant, and low default rate are more likely to be classified as bachelor degree predomint school. This imply a positive relationship between the level of education and students' wealth situation-students in school offering high degree are more likely to come from wealthier family.

Show rules for certificate degree.
```{r}

rules_certificate <- subset(rules_degree, (rhs %in% c("pred_degree=Certificate-degree")))
inspect(sort(rules_certificate,by='lift')[1:5])
plot(sort(rules_certificate,by='lift')[1:5],method="graph", control=list(type="itemsets"))
```
There are 25 rules for certificate degree. Also taking the first rule as an example, institutions with attending students whose debt are low, and have a control type equal to private for-profit are morely to classified as certificate-degree predominant. Looking at the first five rules, we also notice that these institutions usually have graduation with low post-graduation earning, and low debt after completion

Show rules for associate degree
```{r}
rules_associate <- subset(rules_degree, (rhs %in% c("pred_degree=Associate's-degree")))
inspect(sort(rules_associate,by='lift'))
#There are only three rules for associate degree type. Let's look at all of them. Association rules show that net cost for associate-degree predominant school are usually low, and they tend to belong to public type and low percentage of students applying for loan.


```


Show rules for master degree
```{r}
rules_master <- subset(rules_degree, (rhs %in% c("pred_degree=Graduate-degree")))
inspect(sort(rules_master,by='lift'))
```

Show rules for Not-Classified
```{r}
rules_notclassified <- subset(rules_degree, (rhs %in% c("pred_degree=NotClassified")))
inspect(sort(rules_notclassified,by='lift'))
```



## 2.6 Conclusions



#3. Cluster Analysis 
## 3.1 Goal
   The goal of cluster analysis is to find groups (clusters) of rows where
     1. The distance between any two rows in a single cluster is small
     2. The distance between any two rows in different clusters is large
    In our case, the goal of this chapter is to find natural groups of similar institutions and to describe or characterize the institutions in these groups. 
   

## 3.2 Data Preparation
Similar as the last section,we used the data_clean in assignment 1 as our original dataset. Then we deleted the first four columns and renamed the row names with the university ID. 
```{R}
data_c = read.csv("data_clean.csv",
                 header = TRUE, na.strings = 'NA')

#get rid of ID, university name and state columns, rename the rownames as the university ID. 
data.with.rownames <- data.frame(data_c[,-c(1:4)], row.names=data_c[,2])
names(data.with.rownames)
glimpse(data.with.rownames)
```
The data_c dataset now contained 15 variables with 7793 observations. 


In order for a yet-to-be-chosen algorithm to group observations together, we first need to define some notion of (dis)similarity between observations. Considering that most popular choice for clustering like Euclidean distance is only valid for continuous variables, we need to convert the factor variables into binary dummy variables. 

```{R}
# Create the dummy boolean variables using the model.matrix() function.
dummy_preddeg = model.matrix(~PREDDEG_factor-1, data.with.rownames)
dummy_control = model.matrix(~Control_factor-1, data.with.rownames)
dummy_preddeg 
dummy_control

#rename the coloumn names for dummay variables to make them more readable. 
colnames(dummy_preddeg ) <- gsub("PREDDEG_factor","",colnames(dummy_preddeg))
colnames(dummy_control  ) <- gsub("Control_factor","",colnames(dummy_control))
dummy_preddeg 
dummy_control

#Combine the matrix back with the original dataframe.
data_combine_c= cbind(data.with.rownames, dummy_preddeg,dummy_control) 

#git rid of the factor coloumns which have been converted to the dummy variable.
data_ready = data_combine_c[,-c(1:2)]
data_ready
glimpse(data_ready)

#remove the missing values
data_ready %>%
  na.omit(data_ready) %>%
  {.} -> data_clean_c
data_clean_c


```

All the columns in the dataset data_clustering are continuous variables, which are applicable for calculating the (dis)similarity between observations. To ensure the clustering results are able to reveal the valuable information to maximum, we decide to standardize every numeric variable in the dataset. 

```{R}

#delete the 17 col since its all 0 ? correct? 
data_final= data.frame(scale(data_clean_c[,-17]))
names(data_final)

```
The data_final will be used as our final dataset to conduct further clustering analysis. 

## 3.3 Objective

############Clustering Analysis Start#####################

## 3.4 Cluster Analysis process-  K-menas cluster analysis 
There are three main widely used Clustering techniques: K-means, Partitioning Around Medoids (PAM) and DBSCAN. We implemented the three techniques one by one as follows. 

To reproduce the results, we set the seed value to 100 as the code below.
``` {r}
#set random seed to reproduce the results
set.seed(100)
```



To determine the opimum k value for clustering the observations, we drawed a scree plot as follows:
```{R}

# Initialise ratio_ss
ratio_ss <- rep(0, 7)

# Finish the for-loop
for (k in 1:7) {
  # Apply k-means to data_final: data_km
   data_km <- kmeans(data_final, k, nstart = 20)
  # Save the ratio between of WSS to TSS in kth element of ratio_ss
  ratio_ss[k] <- data_km$tot.withinss / data_km$totss
}

# Make a scree plot with type "b" and xlab "k"
plot(ratio_ss, type = "b", xlab = "k")


## choose K, find k that minimizes WSS: scree plot
##tot.withiss   : wss
## betweenss: bss


```

from the scree plot, we can tell that the plot shows a considerable drop for k equal to 3. We will choose k=3 as out optimal k value. 

Then we use kmeans function with the cluter number of 3. 
```{R}
# Cluster the data using k-means: data_km2. 3 clusters, repeat 20 times

km_result <- kmeans(data_final, 3, nstart = 20)
names(data_final)
km_result$cluster

#interpretation with descriptive statistics: the summary data for the 3 resulting clusters
data_final%>%
  mutate(cluster = factor(km_result$cluster)) %>%
   {.} -> data_km

km_summary = data_km %>%
  group_by(cluster) %>%
    do(the_summary = summary(.)) 

km_summary$the_summary
```


```{r}
# Plot the observations with Color using clusters

##ggplot
data_km %>% 
  ggplot(aes(x=MD_EARN_WNE_P6, y=PCTPELL)) + 
  geom_point(aes(color=cluster, size=2)) + 
  guides(size=FALSE)


```


```{r}
names(data_km)

data_km %>%
  ggplot(aes(x = cluster,
             y = MD_EARN_WNE_P6)) +
  geom_boxplot(aes(fill = cluster)) +
  xlab("Cluster") +
  ylab("Median Earning six years after entry") +
  scale_fill_discrete(name = 'Cluster')

qplot(data = data_km,
      x = MD_FAMINC,
      y = MD_EARN_WNE_P6,
      color = cluster,
      xlab = 'Median Family Income',
      ylab = "Median Earning six years after entry")



#Silhouette width
cluster3.dist.mat = daisy(data_)
cluster.sil3 = silhouette(x = as.numeric(data_clustering_scale$kmeans.cluster3),
                          dist = cluster3.dist.mat)
plot(cluster.sil3)
summary(cluster.sil3)
#Dunn index
dunn(dist = dist(data_clustering_scale),
     clusters = as.numeric(kmeans(data_clustering_scale,3)$cluster))


```


##3.5 Cluster Analysis process - Partitioning around mediods (PAM)
```{r}

# Calculate silhouette width for many k using PAM
# see mixed links online

sil_width <- c(NA)

for(i in 2:10){
  pam_fit <- pam(data_final,
                 diss = TRUE,
                 k = i)
  sil_width[i] <- pam_fit$silinfo$avg.width
}

# Plot sihouette width (higher is better)

plot(1:10, sil_width,
     xlab = "Number of clusters",
     ylab = "Silhouette Width")
lines(1:10, sil_width)

pam_fit <- pam(data_final, diss = TRUE, k = 2)
names(pam_fit)
pam_fit$clustering

data_final%>%
  mutate(pam_cluster = factor(pam_fit$clustering)) %>%
   {.} -> data_pam

pam_summary = data_pam %>%
  group_by(pam_cluster) %>%
    do(the_summary = summary(.)) 

pam_summary$the_summary

data_pam %>% 
  ggplot(aes(x=MD_EARN_WNE_P6, y=PCTPELL)) + 
  geom_point(aes(color=pam_cluster, size=2)) + 
  guides(size=FALSE)



```
##3.6 Cluster Analysis process - DBSCAN

```{R}

library(fpc)

dbscan.result = dbscan(data=data_final,eps=0.42,MinPts=10)
dbscan.result$cluster




```


##3.7 Cluster Analysis process- Hierachical 
```{r}

hclust_result = hclust(dist(data_final))
plot(hclust_result, label=data_final$NPT4_Pulic)

data_final%>%
  mutate(hc_cluster = factor(cutree(hclust_result,k=3))) %>%
   {.} -> data_hc

names(data_hc)


#ggplot
data_hc %>% 
  ggplot(aes(x=MD_EARN_WNE_P6, y=PCTPELL)) + 
  geom_point(aes(color=hc_cluster, size=2)) + 
  guides(size=FALSE)


```



## 3.8 Cluster Validation and Comparison 

```{r}
rownames(data_final) = 1:4528
methods.vec = c("hierarchical","kmeans","pam")

clValid.result = clValid(data_final,
                         3:6,
                         clMethods=methods.vec,
                         validation="internal",
                         maxitems = 2000000)
rownames(data_clustering_scale) = 1:nrow(data_clustering_scale)
data.clValid = clValid(apply(data_clustering_scale,2,as.numeric),
                       nClust = 3:6,
                       clMethods = c("kmeans","pam", "hierarchical","agnes"),
                       validation = 'internal',
                       maxitems = 2000000)
summary(data.clValid)



summary(data.clValid)


summary(clValid.result)
```


## 3.9 Findings and Conlusions 
#4 Further Studies


