---
title: "MA710-A2- College Scorecard Association Rules & Cluster Analysis"
author: "Xiang Li, Xue Zhou"
date: "March 21, 2017"
output: html_document
---

# Table of Contents
* 1.[Introduction](#Introduction)
* 2.[Association Rule Analysis](#AR)
  * 2.1 [Goal](#AR_goal)
  * 2.2 [Data Preparation](#AR_dp)
  * 2.3 [Objective](#AR_objective)
  * 2.4 [Association Rule Analysis - Median earning six years after entry](#AR_md6)
  * 2.5 [Conclusions -  Association Rules Analysis for Median earning six years after entry](#2.5)
  * 2.6 [Association Rule Analysis - Predominant Degree](#2.6)
  * 2.7 [Conclusions -  Association Rules Analysis for Predominant Degree](#2.7)
* 3.[Cluster Analysis](#3)
  * 3.1 [Goal](#3.1)
  * 3.2 [Data Preparation](#3.2)
  * 3.3 [Objective](#3.3)
  * 3.4 [Cluster Analysis -K means](#3.4)
  * 3.5 [Cluster Analysis -PAM](#3.5)
  * 3.6 [Cluster Analysis -Hierarchical](#3.6)
  * 3.7 [Cluster Validation and Comparison](#3.7)
  * 3.8 [The Interpretation for 3 cluster hierachical solutions](#3.8)
  * 3.9 [Findings and Conclusions](#3.9)
* 4 [Further Studies](#4)
       


```{r setup, include=FALSE,warning=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message = FALSE)
```

# 1. Introduction<a id="Introduction"></a>
At this stage, we will use association rule mining and cluster analysis to analyze the College Scorecard Data. The report mainly consists of two parts. In the first part, we used the association rule mining to determine the frequent co-occurring associations among college observations and identified the most important relationships. In the second part, we used cluster analysis to obtain clusters of similar colleges, which share the common characterests. 
    
# 2. Association Rule Analysis<a id="AR"></a>
## 2.1. Goal<a id="AR_goal"></a>
At the first stage(Assignment1), we've explored the relationship between pairs of variables in the College Scorecard data set. In this analysis, we mainly focused on two variables which interest us most: the students' post-graduation earnings and the institutions' predominant degree types. We used association rule mining technique to explore relationship between post-graduation earnings, predominant degree and the other variables. The clean data set we obtained in Assignment 1 -  ```data_clean.csv``` is our starting data set in this report. 
      
  
## 2.2 Data Preparation<a id="AR_dp"></a>
First, we loaded all the packages required for analysis as below and imported the data set.  
```{r}
  library(readr)
  library(dplyr)
  library(ggplot2)
  library(tidyr)
  library(ggvis)
  library(plyr)
  library(arules)
  library(arulesViz)
  library(cluster)
  library(clValid)

  data = read.csv("data_clean.csv",
                   header = TRUE, na.strings = 'NA')
```
  As seen from the code block, ```data``` is the original data set we used in this report.
  
   Considering the column X, UNITID, INSTNM_factor and MD_EARN_WNE_P10 are not related to our analysis goal, we removed these four variables. Then we renamed the column names, making them easy to understand.
```{r}
  data_asso = data[,-c(1,2,3,17)]
  #data_asso = na.omit(data_asso)
  data_asso %>% dplyr::rename(state = STABBR_factor,
                       pred_degree = PREDDEG_factor,
                       control = Control_factor,
                       net_cost = NPT4_COMBINE,
                       per_independent = DEP_STAT_PCT_IND,
                       per_1generation = PAR_ED_PCT_1STGEN,
                       median_family_inc =MD_FAMINC,
                       per_pell = PCTPELL,
                       per_loan = PCTFLOAN,
                       debt_grad = GRAD_DEBT_MDN,
                       debt_non_grad = WDRAW_DEBT_MDN,
                       per_app_greater2 = APPL_SCH_PCT_GE2,
                       median_earning_6years = MD_EARN_WNE_P6,
                       repayment_rate = RPY_3YR_RT_SUPP,
                       default_rate = CDR3) %>%
                       {.} -> data_asso
  
```
The ```data_asso``` is the resulting data set. 
  
  
To make the data set applicable for association rules mining, we need to encode all the numeric variables into categorical variables with appropriate levels. We used the following function *make.ntiles* to perform this conversion.
```{r}
  # create factor variables from all of the numeric variables you chose to work with using the make.ntiles function.
  
  make.ntiles = function (inputvar, n) {
    inputvar %>%
  quantile(.,
  (1/n) * 1:(n-1),
               na.rm=TRUE
      ) %>%
  c(-Inf, ., Inf) %>% cut(inputvar,
  breaks=.,
  paste("Q", 1:n, sep="") )
  }
```
  With the above function, we were able to convert each numeric variable into a factor variable with n levels. In our case, we encoded all the numeric variables into 3 categories, thus n =3. 

  
  Then we applied the ```make.ntiles``` function on every numeric column. After the variables were encoded, we combined the them with the original three categorical variables, and thus generated a complete data set. 
```{r}
  data_asso %>% 
    sapply(.,is.numeric) %>%
      data_asso[,.] %>%
        apply(.,make.ntiles, n=3, MARGIN = 2) %>%
           as.data.frame()   %>%
               {.} -> data_num
  data_clean = cbind(data_num,data_asso[,c(1,2,3)])
```
  The data set ```data_clean``` is the resulting data set from the above code block. In terms of the values in the encoded variables, the Q1 is representing the range of the low value, the Q2 is representing the range for the medium value and the Q3 is representing the range of the high value.
  
  
## 2.3 Objective<a id="AR_objective"></a> 
  Specifically, we had two two objectives in this part: 
        1. To identify the attributes which have notable relationships with the post-graduation earnings so that these variables can be used to predict the post-graduate earnings. 
        2. To determine the attributes which can describe the colleges based on the different predominant degree types.
      
## 2.4 Association Rule Analysis - Median earning six years after entry<a id="AR_md6"></a>
  We used the following codes to generate the association rules for the ```data_clean``` data set. We used the parameter, appearance and control parameters of the ```apriori``` command to set the requirements for the generated rules. The appearance parameter requires that the consequent (RHS) in the generated rules should contain at least one level of the variable ```median_earning_6years```. The parameter parameter requires that all the generated rules must have the support value greater than 0.05 and the confidence value greater than 0.8. The minlen and maxlen parameters specify that all generated rules should have a length between 2 to 4. 
```{r}
  #filter the rhs to the MD_EARN_WNE_P6.F column.
  apriori.appearance_e = list(rhs=c('median_earning_6years=Q1','median_earning_6years=Q2','median_earning_6years=Q3'), default='lhs')
  apriori.parameter_e = list(support=0.05,
                           confidence=0.8,minlen=2, maxlen=4)
  apriori.control_e = list(verbose=FALSE)
  rules_earning = apriori(data_clean,   
                   parameter=apriori.parameter_e,
                   appearance=apriori.appearance_e,
                   control=apriori.control_e)
  length(rules_earning)
```
  In total, there are 95 rules generated, based on the parameters we specified before. 
  
  
   Then we would like to determine the redundancies of the rules. A rule is considered redundant if there exist more general rules with a higher predictive power. Considering the redundant rules provide no extra information in addition to the valid rules, we only kept the rules which were not redundant. We used ```lift``` measure to determine the redundant rules. A frequency plot for each item was also analyzed in this session.
```{r}
  rules.sorted <- sort(rules_earning, by="lift") 
 
  ## redundant rules
  rules_redundant = rules.sorted[is.redundant(rules.sorted)]
  rules_redundant 
  
  ## non-redundant rules
  rules_pruned = rules.sorted[!is.redundant(rules.sorted)] 
  rules_pruned 
  
  ##The number of rules associated with rhs of median_earning_6years=Q2
  rules_Q2 = subset(rules_pruned, subset = items %in% "median_earning_6years=Q2")
  length(rules_Q2)
  
  
  ##The number of rules associated with rhs of median_earning_6years=Q3
  rules_Q3 = subset(rules_pruned, subset = items  %in% "median_earning_6years=Q3")
  length(rules_Q3)
  
  #Item frequency PLot
  itemFrequencyPlot(items(rules_pruned), topN=25, cex.names=.6)
```
   From the output, there are 8 redundant rules. We would only focus on the remaining 87 rules. Among the 87 valid rules, 84 of them have the ```RHS``` of ```median_earning_6years=Q3```, 3 of them have the ```RHS``` of ```median_earning_6years=Q2```, while no rules contain the ```RHS``` of  ```median_earning_6years=Q1```. Based on the Item Frequency graph, it is also obvious that the item ```median_earning_6_years = Q3``` has the highest item frequency, which is more than 80%.
  
  
With the following code, we sorted the resulting rules by ```support```. We specifically inspected the top 5 rules. A balloon graph visualizing the top 5 rules is also displayed. 
```{r}
  #sort by support 
  rules_support = sort(rules_pruned,by="support", decreasing = T)
  inspect(rules_support[1:5])
  
  
  plot(head(sort(rules_pruned, by="support"), 5),
    method="grouped")
  
```

Based on the output, the rules with the highest support value all have ```RHS``` as ```median_earning_6years=Q3```. If we take the first rule as an example, the support value of 0.1256 means that the frequency of the item set in this rule is 12.56%, which means that in the data set, 12.56% of the universities have high median family income, high debt amount before graduation, high repayment rate and high median earnings 6 years after they were enrolled; The confidence value is 0.8091, indicating that 80.91% of the universities whose students have high median family income, high debt amount before graduation and high repayment rate are the universities whose students have high median earning 6 years after the enrollment; The lift value of 3.23 means that universities qualified for both antecedent conditions(```LHS```) and consequent conditions(```RHS```) are 223% higher than we would expect if the antecedent and the consequent are independent. The high value of confidence and lift indicates that the rule has a overall good performance.  
  
Then we ordered the pruned 87 rules by confidence and pulled out the top 5 rules. The code below also requsted a balloon graph visualizing these 5 rules.  
```{r}
  rules_confidence = sort(rules_pruned,by="confidence", decreasing = T)
  inspect(rules_confidence[1:5])
  
  plot(head(sort(rules_pruned, by="confidence"), 5),
    method="grouped")
  
```

From the balloon graph, we can tell that the 5 resulting rules have a relative high support value and the rule with item ```per_pell=Q1``` has the highest lift value. Generally speaking, a high confidence value indicates that the frequency of the if/then statement in this rule is high. If we take the first rule as an example, the support value of 0.9094 means that 90.94% of the universities who have a low percentage in Pell grant, high value in debt_non_grad and low value in default rate are the universities with a high median earning value. The high value of lift (3.63) also indicates that the relationship between the antecedent(```per_pell=Q1,debt_non_grad=Q3, default_rate=Q1```) and the consequent(```median_earning_6years=Q3```) is much more significant than would be expected if the two sets are independent. 
  
  
Lastly, we sorted the rules by lift value and futher inspected the top 5 rules. 
```{r}
  rules_lift = sort(rules_earning,by="lift", decreasing = T)
  inspect(rules_lift[1:5])
```
From the output, all the five rules have the lift value large than 1, which indicates that all five rules are useful in finding the consequent item sets(```RHS```). Moreover, the rule with the highest lift value is identical with the rule with the highest confidence value(as we can tell from the previous session), which means that there exists a notable relationship between antecedent items(low Pell percentage value, high debt before graduation and low default rate value) and consequent item(high median post-graduation earnings after enrollment) within this rule. 
  
  
## 2.5 Conclusions -  Association Rules Analysis for Median earning six years after entry<a id="2.5"></a> 
Based on the above analysis, we can conclude that if a college has a low percentage of Pell Grant students, a low default rate, a high repayment rate and a student-base who have a well-educated family background with high family income, its students are more likely to have a high earning after graduation. Thus we are able to identify some potential variables affecting students' post-graduation earnings in a college: percentage of pell grant students, default rate, repayment rate, family income and the percentage of the first-generation students. 

  
## 2.6 Association Rule Analysis - Predominant Degree<a id="2.6"></a>
  The second part of association rule analysis was focused on the relationship between predominant degree type and the other variables. As the codes spcified, we limited the ```RHS``` to the three levels of the predominant degree type. Additionally, we set the minimum support to 0.1, which means that the algorithm will only keep the rules with a frequency greater than 10%. Likewise, the minimum confidence is set to 0.8, which means that we will only keep the rules with the conditional probability greater than 0.8. 
```{r}
  apriori.appearance = list(rhs = c("pred_degree=Bachelor's-degree","pred_degree=Certificate-degree",
                                    "pred_degree=Associate's-degree","pred_degree=NotClassified",
                                    "pred_degree=Graduate-degree"),
                            default = 'lhs')

  apriori.parameter = list(support = 0.10,
                            confidence =0.8)
  apriori.control = list(verbose = FALSE)
  rules_degree = apriori(data_clean,
                  parameter = apriori.parameter,
                  appearance = apriori.appearance,
                  control = apriori.control)
  length(rules_degree)  
```
In total, we got 225 association rules. 

Like what we did in the 2.4 session, we used ```lift``` measure to determine and trim the redundant rules. 
```{r}
  
  rules_degree_sorted <- sort(rules_degree, by="lift") 
  length(rules_degree_sorted)
  
  ## redundant rules
  rules_degree_redundant = rules_degree_sorted[is.redundant(rules_degree_sorted)]
  
  ## non-redundant rules
  rules_degree_pruned = rules_degree_sorted[!is.redundant(rules_degree_sorted)] 
  
  rules_degree_redundant 
  rules_degree_pruned
```
After removing the redundant rules, we got the remaining 195 rules. We would further analyze these rules in the following sections.
  
  
We first sorted all 195 rules by the lift value. If the lift value equals to 1, it would imply that the probability of the occurrence of antecedent(```LHS```) and the probability of occurrence of the consequent(```RHS```) are independent. In another word, the higher the lift value is, the more likely the ```LHS``` and ```RHS```` are associated.
```{r}
  inspect(sort(rules_degree_pruned,by='lift')[1:5])
```
Let's take the first rule as an example. A school with students of low percentage of first generation value, high family income, low percentage of pell grant, high percentage of student applying to more than one school, high percentage of repayment rate is more likely to be a bachelor-degree predominant institution. Specifically, the value of support 0.102 means that about 10% of schools satisfy all items in the antecedent and the consequent. The confidence value of 0.97 implies that about 97% of schools' predominant degree is Bachelor degree, given that those schools have satisfied all the items in the antecedent. A lift as high as 3.5 shows that the relationship between the antecedent and the consequent is more significant than would be expected if the two sets were independent. the large value of the lift ratio in this case indicates a strong association between the antecedent items and the consequent items. 

  
Then we looked at the rules sorted by confidence. Likewise, we would only focus on the top 5 rules. 
```{r}
  inspect(sort(rules_degree_pruned,by='confidence')[1:5])
```
Based on the output, the top rule with the highest confidence value is identical with the top rule with the highest  lift value in the previous section. A rule with a high confidence and lift value is a rule with a good overall performance. Basically, all these 5 rules hava a good overall performance. 
  
  
With the folling code block, we looked at rules sorted by support value and extracted the top 5 rules as well.  
```{r}
  inspect(sort(rules_degree_pruned,by='support')[1:5])
```
A support value of 0.19 means that among all the college records, about 20% are private for-profit, certificate predominant and contain students with low debt amount. A high support value indicates that this rule will be applicable to a larger number of collges. As a result, all these five rules would be helpful, considering the high support value.  


  
In the follwing session, we used some visualizations to better understand the total 195 rules. We drew the scatter plot to visualize the relationships among support, confidence and lift. 
```{r}
  plot(rules_degree_pruned, method = NULL, measure = "support", shading = "lift",
        interactive = FALSE, data = NULL, control = NULL)
```

  The plot shows a negative relationship between confidence and support. However, the relationship between lift and the other two is not obvious.
  
  
In order to profile the institutions under each predominant degree type, we segmented the association rules by their ```RHS```, and then mainly focused on the top 5 rules. 
```{r}
  rules_bachelor <- subset(rules_degree_pruned, (rhs %in% c("pred_degree=Bachelor's-degree")))
  rules_bachelor
  inspect(sort(rules_bachelor,by='lift')[1:5])
  plot(sort(rules_bachelor,by='lift')[1:5],method="graph", control=list(type="itemsets"))
```

   Based on the output, 185 of the total 195 rules have RHS of ```pred_degree=Bachelor's-degree```, which indicates that most rules are implying the associations between the collges with predominant degree Bachelor degree and other characteristics. After sorting the rules by lift, we were able to further analyze the rules with 5 highest lift values. Take the first rule as example, we could conclude that the schools, with students whose family has a high income,with a low percentage of students receiving Pell grant and a low default rate, are those whose predominant degree is bachelor degree. Moreover, the high lift value of 3.56 indicates that the relationship between the antecedent and the consequent is more significant than would be expected, given that the two sets are independent. 
  

## 2.7 Conclusions - Association Rules Analysis for Predominant Degree<a id="2.7"></a>

  As a conclusion, all the rules we found in this session were describing the institutions whose predominant degree are either bachelor or certificate. No explicit association was detected between other degree types and the variables. In the institutions with predominant degree bachelor, the students are more likely to from a wealthy and  well-educated family, more likely to take more debt for their education, and less likely to apply for Pell grant. 

   Moreover, a certificate-predominant university is more likely to be a private for-profit school. Students in these schools usually have lower debt amount before graduation and lower earnings six years after graduation, compared to the schools with other degree types. 
  
   Additionally, the above analysis identified some attributes which might be helpful in profiling a school's predominant degree type: family income, family education background, Percentage of Pell Grant, control type, debt amount before graduation and repayment rate. 
  
  
# 3. Cluster Analysis<a id="3"></a>
## 3.1 Goal<a id="3.1"></a>
  The goal of the Cluster Analysis is to find natural groups of similar institutions and to characterize the institutions in these groups. 
     
  
## 3.2 Data Preparation<a id="3.2"></a>
  Similar to the last section, we used the ```data_clean``` in assignment 1 as the original data set. Then we deleted the first four columns and renamed the row names with the university ID. 
```{R}
  data_c = read.csv("data_clean.csv",
                   header = TRUE, na.strings = 'NA')
  
  #get rid of ID, university name and state columns, rename the rownames as the university ID. 
  data.with.rownames <- data.frame(data_c[,-c(1:4)], row.names=data_c[,2])
  glimpse(data.with.rownames)
```
  The ```data_c``` data set now contained 15 variables with 7793 observations. 
  
  
  Considering that most clustering methods, like Euclidean distance, are only valid for continuous variables, we converted the factor variables into binary dummy variables as the following code.  
```{R}
  # Create the dummy boolean variables using the model.matrix() function.
  dummy_preddeg = model.matrix(~PREDDEG_factor-1, data.with.rownames)
  dummy_control = model.matrix(~Control_factor-1, data.with.rownames)
```
   The matrix ```ummy_preddeg``` and ```dummy_control``` contain the 5 binary dummy variables which were converted from the factor variables ```PREDDEG_factor``` and ```Control_factor```. 
  
  
   In order to make the converted dummy variables easy to understand, we renamed the 7 new columns with the appropriate names and combined them with the original 13 continuous variables. Considering the data set contained some missing values, we removed the records with any missing cells from the data set. 
```{r}
  #rename the coloumn names for dummay variables to make them more readable. 
  colnames(dummy_preddeg ) <- gsub("PREDDEG_factor","",colnames(dummy_preddeg))
  colnames(dummy_control  ) <- gsub("Control_factor","",colnames(dummy_control))
  
  #Combine the matrix back with the original dataframe.
  data_combine_c= cbind(data.with.rownames, dummy_preddeg,dummy_control) 
  
  #git rid of the factor coloumns which have been converted to the dummy variable.
  data_ready = data_combine_c[,-c(1:2)]
  
  #remove the missing values
  data_ready %>%
    na.omit(data_ready) %>%
    {.} -> data_clean_c
  
```
  The clean data set was stored in the data set ```data_clean_c```. All the columns are continuous variables, which are applicable for calculating the (dis)similarity between the observations. 
  
  
   To ensure the same weight among different variables, we standardized the numeric variables in the data set. Given the fact that the 17th column only has the 0 value, which means that this binary variable contains no valuable informaiton, we removed this column. 
```{R}
  #delete the 17 col since its all 0 
  data_final= data.frame(scale(data_clean_c[,-17]))
```
   The ```data_final``` was our final data set, which contains 4528 observations and 20 columns. 
  
  
## 3.3 Objective<a id="3.3"></a>
   Our objective for cluster analysis is to group institutions together into certain clusters that share similar characteristics as determined by several measures of association. Specifically, we will take into account the following cluster analysis approaches: K-means, Partitioning Around Medoids and Hierarchical . Based on the existing candidate resulting clusters, we will use clValis function to validate the best cluster group. After the best clusters are finalized, we will conduct conduct descriptive analysis to describe the clusters of interest and use 2-dimensional scatter plot to visualize the associations between pairs of variables from the perspective of cluster assignment.
  
## 3.4 Cluster Analysis process - K-menas<a id="3.4"></a>
  To reproduce the results, we set the seed value to 100 as the code below.
```{r}
  set.seed(100)
```
    
  
  To determine the optimum k value for clustering the observations, we dragged the scree plots to display the ratio of WSS and TSS and the solutions from 1 cluster to 7 clusters. 
```{R}
  # Initialise ratio_ss
  ratio_ss <- rep(0, 7)
  
  # Finish the for-loop
  for (k in 1:7) {
    # Apply k-means to data_final: data_km
     data_km <- kmeans(data_final, k, nstart = 20)
    # Save the ratio between of WSS to TSS in kth element of ratio_ss
    ratio_ss[k] <- data_km$tot.withinss / data_km$totss
  }
  # Make a scree plot with type "b" and xlab "k"
  plot(ratio_ss, type = "b", xlab = "k")
```
    From the scree plot, we can tell that the plot shows a considerable drop for k equals to 3. We will choose k=3 as our optimal k value. 
  
  
  Then we used k means function with the cluster number of 3, repeating 20 times. 
```{R}
  km_result <- kmeans(data_final, 3, nstart = 20)
  
  data_final%>%
    mutate(cluster = factor(km_result$cluster)) %>%
     {.} -> data_km
  
```
  The corresponding cluster solution is saved in a new column cluster. The new data set is named with data_km.
  
  To better understand the frequency of each cluster, we used the dplyr function to produce the corresponding summary statistics.
```{R}
  
  data_km %>%
    group_by(cluster) %>%
       dplyr::summarise(COUNT = n()) 
```
  From the output, the number of membership for cluster 1, 2, 3 are 946, 1440 and 2142 respectively. 
  
  Then we created some visualizations to help us better understand the relationships between variables under each cluster. We first plotted median earning after six years against percentage of Pell grant for each cluster.  
```{r}
  # Plot the observations with Color using clusters
  data_km %>% 
    ggplot(aes(x=MD_EARN_WNE_P6, y=PCTPELL)) + 
    geom_point(aes(color=cluster)) + 
    guides(size=FALSE)
  
```
  
  From the scatter plot, we can tell that the institutions in cluster 1 and 3 have a relative high value of the percentage of Pell-grant receiving students and a low value of the median earnings 6 years after the enrollment. The distribution for the cluster 3 observations is sparse compared to the distributions of the other 2 cluster observations. 
  
  
  Then we look at the earning distribution for each cluster by creating a box plot. 
```{r}
  names(data_km)
  data_km %>%
    ggplot(aes(x = cluster,
               y = MD_EARN_WNE_P6)) +
    geom_boxplot(aes(fill = cluster)) +
    xlab("Cluster") +
    ylab("Median Earning six years after entry") +
    scale_fill_discrete(name = 'Cluster')
```
  
  
  It seems that institutions in cluster 1 have student with higher post-graduation earning than cluster 2 and 3. Cluster 2 and cluster 3 have similar earning range except that the distribution of schools in cluster 2 has a greater dispersion. 
  
  Next, we plotted median family income against median earning for each cluster.
```{r}
  qplot(data = data_km,
        x = MD_FAMINC,
        y = MD_EARN_WNE_P6,
        color = cluster,
        xlab = 'Median Family Income',
        ylab = "Median Earning six years after entry")
```
  
  
  The plot shows that the median earnings generally increase as median family income increase. But as the family income increase to a certain point, it doesn't have much impact on median earnings after six years.
  
  
## 3.5 Cluster Analysis process - Partitioning around mediods (PAM)<a id="3.5"></a>
  
  To determine an optimal number of clusters for Partitioning around mediods, we calculated the  silhouette width  for cluster numbers from 2 to 10 and plotted a scree plot. 
```{r}
  # Calculate silhouette width for many k using PAM
  
  sil_width <- c(NA)
  
  for(i in 2:10){
    pam_fit <- pam(data_final,diss=FALSE,
                   k = i)
    sil_width[i] <- pam_fit$silinfo$avg.width
  }
  
  plot(1:10, sil_width,
       xlab = "Number of clusters",
       ylab = "Silhouette Width")
  lines(1:10, sil_width)
  
```
  
  
  From the scree plot, we can see the Silhouette Width values are overall low for different number of clusters while the highest value is 0.32 when the number of clusters equals to 2. The low value for Silhouette Width indicates a weak structure for the clustering groups, so let's consider other approaches to see if they perform better. 
  
  
## 3.6 Cluster Analysis process - Hierarchical<a id="3.6"></a> 
  We used the dist function to calculate the Euclidean distance for our observations and then used hclust function to cluster the observations. 
```{r}
  hclust_result = hclust(dist(data_final))
  plot(hclust_result, label=data_final$NPT4_Pulic)
```


A dengogram was also requested with the above code block. Due to the large size for our data set, there is not much useful information we could extract from the bottom part of the dengogram. However, there are clearly two very distinct groups at the top of the dengogram. It looks like either two or three groups might be an interesting place to start investigating. 

  
  We produced the cluster memberships for both two and three clusters as follows: 
```{r}
  data_final %>%
    mutate(hc_cluster2 = factor(cutree(hclust_result,k=2)),hc_cluster3 =factor(cutree(hclust_result,k=3)) ) %>%
     {.} -> data_hc
```
  The solution was recorded respectively in a new column hc_cluster2 and hc_cluster3, the updated data set was named with data_hc. Before we dig deeper into the interpretations for cluster solutions, we will use *clValid* function in the following section to validate and select the optimum cluster methods. 

  
## 3.7 Cluster Validations and Comparisons<a id="3.7"></a> 
  
To validate different clustering methods and choice of number of clusters, we ran a cluster validation using *clValid* function. It allows us to compare different clustering evaluation metrics under different clustering approach and number of clusters.
```{r}
  #rownames(data_final) = 1:4528
  methods.vec = c("hierarchical","kmeans","pam")
  clValid.result = clValid(data_final,
                           2:5,
                           clMethods=methods.vec,
                           validation="internal",
                           maxitems = 2000000)
  
  summary(clValid.result)
```

The result shows that the best choice of number of cluster is 2, and the best method is hierarchical. Moreover, we can conclude that the hierarchical solution outperforms the k-means solution when the number of the cluster is 3. 

  
In the following session, we first analyzed the 2-cluster analysis in details. 
```{r}
  #two cluster solutions 
  data_hc %>%
    group_by(hc_cluster2) %>%
       dplyr::summarise(COUNT = n()) 
```
From the output for the two cluster solutions, the first cluster has 4524 observations while the second cluster has only 4 observations. With the only 4 observations in the second cluster, we can get very limited information from 2-cluster solution. So we decided to use the hierarchical method with 3 cluster solutions as our optimal cluster choice. 
  
  
## 3.8 The Interpretation for 3 cluster hierachical solutions<a id="3.8"></a> 
We used the following codes to generate the histogram for Hierarchical 3 Cluster Solutions.
```{r}
#three cluster solutions 
data_hc %>%
    group_by(hc_cluster3) %>%
       dplyr::summarise(COUNT = n())


data_hc %>%
  group_by(hc_cluster3) %>%
     dplyr::summarise(COUNT = n()) %>%
       ggplot(aes(x = hc_cluster3, y = COUNT)) + geom_bar(stat = "identity") + ggtitle('Histogram for Hierachical 3 Cluster Solutions') + theme(plot.title = element_text(family = "Trebuchet MS", color="#666666", face="bold", size=12, hjust=0.5)) 

```


Based on the output, the first cluster has 2992 observations, the second cluster has 1532 observations while the third cluster has 4 observations. 



Considering the third cluster only has 4 observations, we would like to directly interpret the this cluster by looking at the labels of this cluster. 
```{r}
#select the cluster 3 membership from the original dataset

cluster3 = subset(data_c, UNITID %in% rownames(subset(data_hc,hc_cluster3==3)))[,-1]
cluster3[,1:5]
```
 Based on the output, we can tell that all of these four universities are all Private For-profit with the predominant degree not classified. Attributes make this cluster different from other two clusters are very high percent of pell grant, very high percentage of loan, slightly high percent of first-generation students his predominant degree type (NotClassified). 
 


We calculated each variable's median value aggregated by the three clusters.
```{r}
#three cluster solution summary statistics 
data_hc[,-21]%>%
  group_by(hc_cluster3) %>%
     dplyr::summarise_each(funs(median)) 
```
Besides the profile of cluster 3, we can tell that the cluster 1 are the TITLE IV institutions with low average net price, high percentage of students who are financially independent, high percentage first-generation students, low median family income, high percentage of Pell Grant receiving students, high percent of all federal undergraduate students receiving a federal loan, low median debt for students who have completed, low median debt for students who have not completed, low median income six years after entry, low median income ten years after entry, low repayment rate 3 years after entering repayment, high loan default rate 3 years after entering repayment. There is no obvious shared characteristic in terms of the institution's control type or predominant degree in cluster 1. For the institutions in cluster 2, the sign for most columns' median value are opposite to those in the cluster 1.    


To analyze the variables of interest under the Hierarchical 3 cluster solutions, we used the ggplot functions to plot the visualizations for certain variable(s).  Below is the code to generate the scatter plot between PCTPELL and MD_EARN_WNE_P under the hierarchical 3 cluster solutions.
```{R}
#ggplot
data_hc %>% 
  ggplot(aes(x=MD_EARN_WNE_P6, y=PCTPELL)) + 
  geom_point(aes(color=hc_cluster3)) + 
  guides(size=FALSE) + ggtitle('Scatter Plot for PCTPELL against MD_EARN_WNE_P under Hierachical 3 cluster solutions ') + theme(plot.title = element_text(family = "Trebuchet MS", color="#666666", face="bold", size=8, hjust=0.5)) 
```
  
   From the scatter plot, it is easy to see that the institutions in cluster 1 have a high percentage of students who receive the Pell Grant meanwhile their median earnings 6 years after the enrollment is relative low compared to the records in the cluster 2. 


Likewise, we explored the distributions of earnings for each cluster as below: 
```{r}
data_hc %>%
  ggplot(aes(x = hc_cluster3,
             y = MD_EARN_WNE_P6)) +
  geom_boxplot(aes(fill = hc_cluster3)) +
  xlab("Cluster") +
  ylab("Median Earning six years after entry") +
  scale_fill_discrete(name = 'Cluster') 
```

   It's evident that the the cluster 2 has higher value of earnings than cluster 1 and 3, and about 70% of values in this cluster are above the average earning for all observations. The earning values for cluster 1 and 3 roughly fall in similar range while the cluster 1's observations spread more widely. Moreover, most of the observations in these two clusters are below the average earning value.


We also plotted median family income against earning after six years of entry for each cluster under Hierarchical 3 cluster solutions.
```{r}
qplot(data = data_hc,
      x = MD_FAMINC,
      y = MD_EARN_WNE_P6,
      color = hc_cluster3,
      xlab = 'Median Family Income',
      ylab = "Median Earning six years after entry",
      main = "Scatter plot between Median Earning six years after entry and Median Family Income and")
```
  
    We see that the most observations in cluster 1 have low value for both Median Family Income and Median Earning six years after entry, while the observations in cluster 2 overall have a high value of Median Family Income. However, the Median Earning six years after entry values for the observations in the cluster 2 are not that high as we assumed. 


## 3.9 Findings and Conclusions<a id="3.9"></a> 
To find the intrinsic grouping in the College Scorecard data set, we used K-means, PAM and Hierarchical methods to perform clustering analysis. The resulting clusters from each method were different so we then used metrics such as Connectivity, Dunn Index and Silhouette width to evaluate how well observations were clustered. 

The optimal method and number of clusters we found was Hierarchical method with three clusters. When selecting the best methods and number of clusters to use, we took into account not only the clusters performance represented by evaluation metrics but also human judgement on the usefulness of the clusters. Therefore, we decided to give up the best result returned by computer - Hierarchical with 2 clusters and go with Hierarchical with 3 clusters.

The final three clusters contain 2992 observations, 1532 observations and 4 observations respectively. It's unusual that one cluster only contains 4 observation which may not be general enough to give us some useful insights. In the future, one alternative study we will conduct is to treat these 4 observations as outliers and do another cluster analysis without these 4 rows.

The result shows that institutions from cluster 1 tend to have students from less wealthier background and need more financial aid for their education. These students also make less money after graduation. In contrast, cluster 2 contains institutions whose students come from more educated and wealthier family, and make more money after graduation. 



# 4.Further Studies<a id="4"></a>

The association rules we have extracted from this stage can be used to determine the candidate predictors affecting colleges' post-graduation earnings in the future stages. Likewise, clustering result can also be used as a new input in the predictive model.

In terms of the solution we have selected for the cluster analysis, the results might be improved by taking a further investigation on the 4 observations in cluster 3. The analysis process covered in this paper could be revisited if the 4 observations in the cluster 3 are outliers, thus can be removed. Moreover, a density-based cluster method like DBSCAN can be included in the analysis process as well.




